{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1762134242364,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "fq6InQ2Awdho"
   },
   "outputs": [],
   "source": [
    "#loading data from kaggle api\n",
    "# !kaggle competitions download -c mitsui-commodity-prediction-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1762134271815,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "XOQMURlewvRX"
   },
   "outputs": [],
   "source": [
    "#unzipping data\n",
    "# !unzip mitsui-commodity-prediction-challenge.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2726,
     "status": "ok",
     "timestamp": 1762134275836,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "1CQ6mzNkv-Pv"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "pd.options.display.float_format = '{:,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1762134276326,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "UqUoAcm9wN_H"
   },
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "target_pairs = pd.read_csv(\"../data/target_pairs.csv\")\n",
    "train_labels = pd.read_csv(\"../data/train_labels.csv\")\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "test_labels_lag_1 = pd.read_csv(\"../data/lagged_test_labels/test_labels_lag_1.csv\")\n",
    "test_labels_lag_2 = pd.read_csv(\"../data/lagged_test_labels/test_labels_lag_2.csv\")\n",
    "test_labels_lag_3 = pd.read_csv(\"../data/lagged_test_labels/test_labels_lag_3.csv\")\n",
    "test_labels_lag_4 = pd.read_csv(\"../data/lagged_test_labels/test_labels_lag_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1762134276346,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "uKbElUJi-6Ly",
    "outputId": "862bfd2b-ebf6-4590-f8cd-8a71a59e4dff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                                            target_52\n",
       "lag                                                       1\n",
       "pair      US_Stock_CAT_adj_close - JPX_Gold_Standard_Fut...\n",
       "Name: 52, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = target_pairs.iloc[52,0]\n",
    "lag = target_pairs.iloc[52,1]\n",
    "pair = target_pairs.iloc[52,2]\n",
    "target_pairs.iloc[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1762134276364,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "qGF5H04h-87b",
    "outputId": "0c35b0f5-a8a0-49b0-9403-9a894a6cd2e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US_Stock_CAT_adj_close', 'JPX_Gold_Standard_Futures_Close']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodities = pair.split(' - ')\n",
    "comA = commodities[0]\n",
    "comB = commodities[1]\n",
    "commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1762134276783,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "kvGhB_LE_wp5",
    "outputId": "96c96624-fbf2-4e4e-90e3-db2f54bb898f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>US_Stock_CAT_adj_close</th>\n",
       "      <th>JPX_Gold_Standard_Futures_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1827</td>\n",
       "      <td>382.7670</td>\n",
       "      <td>13,641.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,670.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1829</td>\n",
       "      <td>396.4511</td>\n",
       "      <td>13,661.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1830</td>\n",
       "      <td>395.7047</td>\n",
       "      <td>13,828.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1831</td>\n",
       "      <td>404.4526</td>\n",
       "      <td>13,934.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1956</td>\n",
       "      <td>413.7100</td>\n",
       "      <td>16,090.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1957</td>\n",
       "      <td>411.5722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1958</td>\n",
       "      <td>418.7183</td>\n",
       "      <td>16,221.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1959</td>\n",
       "      <td>429.1564</td>\n",
       "      <td>16,306.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1960</td>\n",
       "      <td>431.0934</td>\n",
       "      <td>16,010.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  US_Stock_CAT_adj_close  JPX_Gold_Standard_Futures_Close\n",
       "0       1827                382.7670                      13,641.0000\n",
       "1       1828                     NaN                      13,670.0000\n",
       "2       1829                396.4511                      13,661.0000\n",
       "3       1830                395.7047                      13,828.0000\n",
       "4       1831                404.4526                      13,934.0000\n",
       "..       ...                     ...                              ...\n",
       "129     1956                413.7100                      16,090.0000\n",
       "130     1957                411.5722                              NaN\n",
       "131     1958                418.7183                      16,221.0000\n",
       "132     1959                429.1564                      16,306.0000\n",
       "133     1960                431.0934                      16,010.0000\n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test[[\"date_id\",comA,comB]]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1762134277012,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "9I-X-L_PAcZx",
    "outputId": "45673498-1d45-4c1c-d501-342c1d509bd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>target_52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1829</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1830</td>\n",
       "      <td>-0.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1831</td>\n",
       "      <td>0.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1832</td>\n",
       "      <td>0.0046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1833</td>\n",
       "      <td>-0.0287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1958</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1959</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1960</td>\n",
       "      <td>0.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1961</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1962</td>\n",
       "      <td>-0.0043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  target_52\n",
       "0       1829        NaN\n",
       "1       1830    -0.0140\n",
       "2       1831     0.0142\n",
       "3       1832     0.0046\n",
       "4       1833    -0.0287\n",
       "..       ...        ...\n",
       "129     1958        NaN\n",
       "130     1959     0.0194\n",
       "131     1960     0.0228\n",
       "132     1961     0.0101\n",
       "133     1962    -0.0043\n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_lag_1[[\"date_id\",\"target_52\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1762134277501,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "nDDwPOAoAg27",
    "outputId": "eb10ee9d-6a9d-48be-e188-7bbb4cece0ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>target_52</th>\n",
       "      <th>label_date_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1830</td>\n",
       "      <td>-0.0140</td>\n",
       "      <td>1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1831</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>1829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1832</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>1830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1833</td>\n",
       "      <td>-0.0287</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1959</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1960</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1961</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1962</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  target_52  label_date_id\n",
       "0       1829        NaN           1827\n",
       "1       1830    -0.0140           1828\n",
       "2       1831     0.0142           1829\n",
       "3       1832     0.0046           1830\n",
       "4       1833    -0.0287           1831\n",
       "..       ...        ...            ...\n",
       "129     1958        NaN           1956\n",
       "130     1959     0.0194           1957\n",
       "131     1960     0.0228           1958\n",
       "132     1961     0.0101           1959\n",
       "133     1962    -0.0043           1960\n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_lag_1[[\"date_id\",\"target_52\",\"label_date_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1762134277726,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "vi4rFawXAuYw",
    "outputId": "52adf050-c969-48ea-ba14-922fc194dfcf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>US_Stock_CAT_adj_close</th>\n",
       "      <th>JPX_Gold_Standard_Futures_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>132.7149</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>132.9177</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>134.7431</td>\n",
       "      <td>4,730.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>136.8728</td>\n",
       "      <td>4,780.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>140.3123</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>1956</td>\n",
       "      <td>413.7100</td>\n",
       "      <td>16,090.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>1957</td>\n",
       "      <td>411.5722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>1958</td>\n",
       "      <td>418.7183</td>\n",
       "      <td>16,221.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>1959</td>\n",
       "      <td>429.1564</td>\n",
       "      <td>16,306.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>1960</td>\n",
       "      <td>431.0934</td>\n",
       "      <td>16,010.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1961 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_id  US_Stock_CAT_adj_close  JPX_Gold_Standard_Futures_Close\n",
       "0           0                132.7149                              NaN\n",
       "1           1                132.9177                              NaN\n",
       "2           2                134.7431                       4,730.0000\n",
       "3           3                136.8728                       4,780.0000\n",
       "4           4                140.3123                              NaN\n",
       "...       ...                     ...                              ...\n",
       "1956     1956                413.7100                      16,090.0000\n",
       "1957     1957                411.5722                              NaN\n",
       "1958     1958                418.7183                      16,221.0000\n",
       "1959     1959                429.1564                      16,306.0000\n",
       "1960     1960                431.0934                      16,010.0000\n",
       "\n",
       "[1961 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train[[\"date_id\",comA,comB]]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1762134278151,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "A4mPYIZxPduT",
    "outputId": "1f88e98a-f803-4bc2-9574-9970748290e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>US_Stock_CAT_adj_close</th>\n",
       "      <th>JPX_Gold_Standard_Futures_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>1829</td>\n",
       "      <td>396.4511</td>\n",
       "      <td>13,661.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>1830</td>\n",
       "      <td>395.7047</td>\n",
       "      <td>13,828.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>1831</td>\n",
       "      <td>404.4526</td>\n",
       "      <td>13,934.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>1832</td>\n",
       "      <td>405.6767</td>\n",
       "      <td>13,912.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>1833</td>\n",
       "      <td>393.0873</td>\n",
       "      <td>13,873.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>1956</td>\n",
       "      <td>413.7100</td>\n",
       "      <td>16,090.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>1957</td>\n",
       "      <td>411.5722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>1958</td>\n",
       "      <td>418.7183</td>\n",
       "      <td>16,221.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>1959</td>\n",
       "      <td>429.1564</td>\n",
       "      <td>16,306.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>1960</td>\n",
       "      <td>431.0934</td>\n",
       "      <td>16,010.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_id  US_Stock_CAT_adj_close  JPX_Gold_Standard_Futures_Close\n",
       "1829     1829                396.4511                      13,661.0000\n",
       "1830     1830                395.7047                      13,828.0000\n",
       "1831     1831                404.4526                      13,934.0000\n",
       "1832     1832                405.6767                      13,912.0000\n",
       "1833     1833                393.0873                      13,873.0000\n",
       "...       ...                     ...                              ...\n",
       "1956     1956                413.7100                      16,090.0000\n",
       "1957     1957                411.5722                              NaN\n",
       "1958     1958                418.7183                      16,221.0000\n",
       "1959     1959                429.1564                      16,306.0000\n",
       "1960     1960                431.0934                      16,010.0000\n",
       "\n",
       "[132 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_date_test = test[\"date_id\"].min()\n",
    "last_date_test = test[\"date_id\"].max()\n",
    "first_date_test_label = test_labels_lag_1[\"date_id\"].min()\n",
    "last_date_test_label = test_labels_lag_1[\"date_id\"].max()\n",
    "\n",
    "start_date = first_date_test_label\n",
    "last_date = last_date_test\n",
    "\n",
    "train_df_filtered = train_df[(train_df['date_id'] >= start_date) & (train_df['date_id'] <= last_date)]\n",
    "train_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1762134278514,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "LBNUUtOCBl-L",
    "outputId": "7f136bb6-ae3b-486c-ea75-40a7a2b80704"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>US_Stock_CAT_adj_close</th>\n",
       "      <th>JPX_Gold_Standard_Futures_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1827</td>\n",
       "      <td>382.7670</td>\n",
       "      <td>13,641.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,670.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1829</td>\n",
       "      <td>396.4511</td>\n",
       "      <td>13,661.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1830</td>\n",
       "      <td>395.7047</td>\n",
       "      <td>13,828.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1831</td>\n",
       "      <td>404.4526</td>\n",
       "      <td>13,934.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1956</td>\n",
       "      <td>413.7100</td>\n",
       "      <td>16,090.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1957</td>\n",
       "      <td>411.5722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1958</td>\n",
       "      <td>418.7183</td>\n",
       "      <td>16,221.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1959</td>\n",
       "      <td>429.1564</td>\n",
       "      <td>16,306.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1960</td>\n",
       "      <td>431.0934</td>\n",
       "      <td>16,010.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  US_Stock_CAT_adj_close  JPX_Gold_Standard_Futures_Close\n",
       "0       1827                382.7670                      13,641.0000\n",
       "1       1828                     NaN                      13,670.0000\n",
       "2       1829                396.4511                      13,661.0000\n",
       "3       1830                395.7047                      13,828.0000\n",
       "4       1831                404.4526                      13,934.0000\n",
       "..       ...                     ...                              ...\n",
       "129     1956                413.7100                      16,090.0000\n",
       "130     1957                411.5722                              NaN\n",
       "131     1958                418.7183                      16,221.0000\n",
       "132     1959                429.1564                      16,306.0000\n",
       "133     1960                431.0934                      16,010.0000\n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test[[\"date_id\",comA,comB]]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1762134278587,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "2R4TKRZINI-3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def generate_log_returns(data, lag):\n",
    "    log_returns = pd.Series(np.nan, index=data.index)\n",
    "\n",
    "    # Compute log returns based on the rules\n",
    "    for t in range(len(data)):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            try:\n",
    "                log_returns.iloc[t] = np.log(data.iloc[t + lag + 1] / data.iloc[t + 1])\n",
    "            except Exception:\n",
    "                log_returns.iloc[t] = np.nan\n",
    "    return log_returns\n",
    "\n",
    "\n",
    "def generate_targets(date: pd.Series ,column_a: pd.Series, column_b: pd.Series, lag: int) -> pd.Series:\n",
    "    a_returns = generate_log_returns(column_a, lag)\n",
    "    b_returns = generate_log_returns(column_b, lag)\n",
    "    return pd.DataFrame({\"date_index\": date, \"target_generated\": a_returns - b_returns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1762134279157,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "bsC2fFGuNXha",
    "outputId": "3a75d404-fd49-46e2-dbdf-d9111ccce577"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_index</th>\n",
       "      <th>target_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>1956</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>1957</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>1958</td>\n",
       "      <td>0.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>1959</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>1960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1961 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_index  target_generated\n",
       "0              0               NaN\n",
       "1              1            0.0052\n",
       "2              2               NaN\n",
       "3              3               NaN\n",
       "4              4            0.0070\n",
       "...          ...               ...\n",
       "1956        1956               NaN\n",
       "1957        1957            0.0194\n",
       "1958        1958            0.0228\n",
       "1959        1959               NaN\n",
       "1960        1960               NaN\n",
       "\n",
       "[1961 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_delta = generate_targets(train[\"date_id\"],train[comA], train[comB], lag)\n",
    "train_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1762134279171,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "5Y4wtcQ_Nv6C",
    "outputId": "5890fcdc-2780-411c-9ced-6dcbae0ea6b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_index</th>\n",
       "      <th>target_generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1827</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1828</td>\n",
       "      <td>-0.0140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1829</td>\n",
       "      <td>0.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1830</td>\n",
       "      <td>0.0046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1831</td>\n",
       "      <td>-0.0287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1956</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1957</td>\n",
       "      <td>0.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1958</td>\n",
       "      <td>0.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1959</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1960</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_index  target_generated\n",
       "0          1827               NaN\n",
       "1          1828           -0.0140\n",
       "2          1829            0.0142\n",
       "3          1830            0.0046\n",
       "4          1831           -0.0287\n",
       "..          ...               ...\n",
       "129        1956               NaN\n",
       "130        1957            0.0194\n",
       "131        1958            0.0228\n",
       "132        1959               NaN\n",
       "133        1960               NaN\n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_delta = generate_targets(test[\"date_id\"],test[comA], test[comB], lag)\n",
    "test_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1762134279431,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "uFYcEgN8PCH1",
    "outputId": "e8602720-6ce7-4a2a-f8b6-fd67a290a8b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>target_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1829</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1830</td>\n",
       "      <td>0.0026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1831</td>\n",
       "      <td>0.0053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1832</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1833</td>\n",
       "      <td>-0.0115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1958</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1959</td>\n",
       "      <td>0.0114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1960</td>\n",
       "      <td>-0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1961</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1962</td>\n",
       "      <td>-0.0050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  target_0\n",
       "0       1829       NaN\n",
       "1       1830    0.0026\n",
       "2       1831    0.0053\n",
       "3       1832    0.0001\n",
       "4       1833   -0.0115\n",
       "..       ...       ...\n",
       "129     1958    0.0028\n",
       "130     1959    0.0114\n",
       "131     1960   -0.0027\n",
       "132     1961    0.0021\n",
       "133     1962   -0.0050\n",
       "\n",
       "[134 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_lag_1[[\"date_id\",\"target_0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1762134279666,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "mI3KiDreQZeq",
    "outputId": "97d9ca6b-8853-42d0-a141-5ce5c11b7252"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>US_Stock_CAT_adj_close</th>\n",
       "      <th>JPX_Gold_Standard_Futures_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1829</td>\n",
       "      <td>396.4511</td>\n",
       "      <td>13,661.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1830</td>\n",
       "      <td>395.7047</td>\n",
       "      <td>13,828.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1831</td>\n",
       "      <td>404.4526</td>\n",
       "      <td>13,934.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1832</td>\n",
       "      <td>405.6767</td>\n",
       "      <td>13,912.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1833</td>\n",
       "      <td>393.0873</td>\n",
       "      <td>13,873.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1956</td>\n",
       "      <td>413.7100</td>\n",
       "      <td>16,090.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1957</td>\n",
       "      <td>411.5722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1958</td>\n",
       "      <td>418.7183</td>\n",
       "      <td>16,221.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1959</td>\n",
       "      <td>429.1564</td>\n",
       "      <td>16,306.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1960</td>\n",
       "      <td>431.0934</td>\n",
       "      <td>16,010.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     date_id  US_Stock_CAT_adj_close  JPX_Gold_Standard_Futures_Close\n",
       "2       1829                396.4511                      13,661.0000\n",
       "3       1830                395.7047                      13,828.0000\n",
       "4       1831                404.4526                      13,934.0000\n",
       "5       1832                405.6767                      13,912.0000\n",
       "6       1833                393.0873                      13,873.0000\n",
       "..       ...                     ...                              ...\n",
       "129     1956                413.7100                      16,090.0000\n",
       "130     1957                411.5722                              NaN\n",
       "131     1958                418.7183                      16,221.0000\n",
       "132     1959                429.1564                      16,306.0000\n",
       "133     1960                431.0934                      16,010.0000\n",
       "\n",
       "[132 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_filtered = test_df[(test_df['date_id'] >= start_date) & (test_df['date_id'] <= last_date)]\n",
    "test_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1762134280091,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "jzlZ5mrCSF0u",
    "outputId": "e359c3aa-5348-4f9b-8964-2232f2947e83"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_id</th>\n",
       "      <th>US_Stock_CAT_adj_close</th>\n",
       "      <th>JPX_Gold_Standard_Futures_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>1829</td>\n",
       "      <td>396.4511</td>\n",
       "      <td>13,661.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>1830</td>\n",
       "      <td>395.7047</td>\n",
       "      <td>13,828.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>1831</td>\n",
       "      <td>404.4526</td>\n",
       "      <td>13,934.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>1832</td>\n",
       "      <td>405.6767</td>\n",
       "      <td>13,912.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>1833</td>\n",
       "      <td>393.0873</td>\n",
       "      <td>13,873.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>1956</td>\n",
       "      <td>413.7100</td>\n",
       "      <td>16,090.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>1957</td>\n",
       "      <td>411.5722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>1958</td>\n",
       "      <td>418.7183</td>\n",
       "      <td>16,221.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>1959</td>\n",
       "      <td>429.1564</td>\n",
       "      <td>16,306.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>1960</td>\n",
       "      <td>431.0934</td>\n",
       "      <td>16,010.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_id  US_Stock_CAT_adj_close  JPX_Gold_Standard_Futures_Close\n",
       "1829     1829                396.4511                      13,661.0000\n",
       "1830     1830                395.7047                      13,828.0000\n",
       "1831     1831                404.4526                      13,934.0000\n",
       "1832     1832                405.6767                      13,912.0000\n",
       "1833     1833                393.0873                      13,873.0000\n",
       "...       ...                     ...                              ...\n",
       "1956     1956                413.7100                      16,090.0000\n",
       "1957     1957                411.5722                              NaN\n",
       "1958     1958                418.7183                      16,221.0000\n",
       "1959     1959                429.1564                      16,306.0000\n",
       "1960     1960                431.0934                      16,010.0000\n",
       "\n",
       "[132 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1762134403063,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "x0r3V3_jTQjB",
    "outputId": "818198b6-09ed-4bf5-89ba-1bfa0b22772c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_filtered.equals(test_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1762134534800,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "4bca2fd5",
    "outputId": "1a6ccbc8-e7aa-4a61-b1e1-a07c8370ab8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 3)\n",
      "(132, 3)\n",
      "Index(['date_id', 'US_Stock_CAT_adj_close', 'JPX_Gold_Standard_Futures_Close'], dtype='object')\n",
      "Index(['date_id', 'US_Stock_CAT_adj_close', 'JPX_Gold_Standard_Futures_Close'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df_filtered.shape)\n",
    "print(test_df_filtered.shape)\n",
    "print(train_df_filtered.columns)\n",
    "print(test_df_filtered.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3536,
     "status": "error",
     "timestamp": 1762134599581,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "FXdAoihJZC2w",
    "outputId": "635ced7c-88de-4768-a4cd-811385bf18df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: (500,)\n",
      "Sample data (first 5): 2020-01-01    98.6172\n",
      "2020-01-02   105.9527\n",
      "2020-01-03   110.5367\n",
      "2020-01-04   108.8454\n",
      "2020-01-05   109.4935\n",
      "Freq: D, Name: price, dtype: float64\n",
      "============================================================\n",
      "STARTING HETEROSCEDASTICITY-AWARE XGBOOST PIPELINE\n",
      "============================================================\n",
      "\n",
      "=== STATIONARITY TESTS ===\n",
      "\n",
      "Augmented Dickey-Fuller Test:\n",
      "  ADF Statistic: -0.5331\n",
      "  p-value: 0.8854\n",
      "  Critical Values: {'1%': np.float64(-3.4438771098680196), '5%': np.float64(-2.867505393939065), '10%': np.float64(-2.569947324764179)}\n",
      "  Result: NON-STATIONARY\n",
      "\n",
      "KPSS Test:\n",
      "  KPSS Statistic: 0.0220\n",
      "  p-value: 0.1000\n",
      "  Critical Values: {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216}\n",
      "  Result: STATIONARY\n",
      "\n",
      "=== MAKING SERIES STATIONARY ===\n",
      "\n",
      "=== STATIONARITY TESTS ===\n",
      "\n",
      "Augmented Dickey-Fuller Test:\n",
      "  ADF Statistic: -0.9713\n",
      "  p-value: 0.7636\n",
      "  Critical Values: {'1%': np.float64(-3.4438771098680196), '5%': np.float64(-2.867505393939065), '10%': np.float64(-2.569947324764179)}\n",
      "  Result: NON-STATIONARY\n",
      "\n",
      "KPSS Test:\n",
      "  KPSS Statistic: 0.0413\n",
      "  p-value: 0.1000\n",
      "  Critical Values: {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216}\n",
      "  Result: STATIONARY\n",
      "\n",
      "=== STATIONARITY TESTS ===\n",
      "\n",
      "Augmented Dickey-Fuller Test:\n",
      "  ADF Statistic: -15.3803\n",
      "  p-value: 0.0000\n",
      "  Critical Values: {'1%': np.float64(-3.4438771098680196), '5%': np.float64(-2.867505393939065), '10%': np.float64(-2.569947324764179)}\n",
      "  Result: STATIONARY\n",
      "\n",
      "KPSS Test:\n",
      "  KPSS Statistic: 0.0097\n",
      "  p-value: 0.1000\n",
      "  Critical Values: {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216}\n",
      "  Result: STATIONARY\n",
      "First differencing sufficient\n",
      "\n",
      "=== FINDING OPTIMAL LAGS ===\n",
      "Optimal lags: [1, 2, 4, 5, 6, 7, 8, 10, 11, 12]\n",
      "\n",
      "=== STL DECOMPOSITION ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "seasonal must be an odd positive integer >= 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 525\u001b[39m\n\u001b[32m    522\u001b[39m pipeline = HeteroscedasticXGBoostPipeline(seasonal_period=\u001b[32m12\u001b[39m, max_lags=\u001b[32m20\u001b[39m)\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# Option 1: Quantile regression (recommended for heteroscedasticity)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m results_quantile = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_quantile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# Option 2: Two-model approach (mean + volatility)\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[38;5;66;03m# results_two_model = pipeline.fit(series, use_quantile=False)\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 466\u001b[39m, in \u001b[36mHeteroscedasticXGBoostPipeline.fit\u001b[39m\u001b[34m(self, series, use_quantile)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28mself\u001b[39m.find_optimal_lags(series_stationary)\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# Step 3: STL decomposition\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstl_decomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries_stationary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[38;5;66;03m# Step 4: Create features\u001b[39;00m\n\u001b[32m    469\u001b[39m features_df = \u001b[38;5;28mself\u001b[39m.create_features(series_stationary, include_volatility=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mHeteroscedasticXGBoostPipeline.stl_decomposition\u001b[39m\u001b[34m(self, series)\u001b[39m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Series too short for seasonal decomposition (need >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m2\u001b[39m*\u001b[38;5;28mself\u001b[39m.seasonal_period\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m stl = \u001b[43mSTL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseasonal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseasonal_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m result = stl.fit()\n\u001b[32m    174\u001b[39m \u001b[38;5;28mself\u001b[39m.stl_components = {\n\u001b[32m    175\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrend\u001b[39m\u001b[33m'\u001b[39m: result.trend,\n\u001b[32m    176\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mseasonal\u001b[39m\u001b[33m'\u001b[39m: result.seasonal,\n\u001b[32m    177\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresid\u001b[39m\u001b[33m'\u001b[39m: result.resid\n\u001b[32m    178\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mstatsmodels/tsa/stl/_stl.pyx:226\u001b[39m, in \u001b[36mstatsmodels.tsa.stl._stl.STL.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: seasonal must be an odd positive integer >= 3"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Heteroscedasticity-Aware XGBoost Pipeline for Commodity Price Prediction\n",
    "Includes: Stationarity testing, STL decomposition, volatility modeling, rolling window CV\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from scipy.stats import boxcox, jarque_bera\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class HeteroscedasticXGBoostPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for commodity price prediction with heteroscedasticity handling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seasonal_period=12, max_lags=20):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        seasonal_period : int\n",
    "            Expected seasonality period (12 for monthly, 252 for daily trading days)\n",
    "        max_lags : int\n",
    "            Maximum number of lags to consider\n",
    "        \"\"\"\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.max_lags = max_lags\n",
    "        self.differencing_order = 0\n",
    "        self.transformation = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.optimal_lags = []\n",
    "\n",
    "        # Models\n",
    "        self.mean_model = None\n",
    "        self.vol_model = None\n",
    "        self.quantile_models = {}\n",
    "\n",
    "        # Results storage\n",
    "        self.stationarity_results = {}\n",
    "        self.heteroscedasticity_results = {}\n",
    "        self.stl_components = None\n",
    "\n",
    "    def test_stationarity(self, series, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Perform ADF and KPSS tests for stationarity\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STATIONARITY TESTS ===\")\n",
    "\n",
    "        # ADF Test (H0: unit root present - non-stationary)\n",
    "        adf_result = adfuller(series.dropna(), autolag='AIC')\n",
    "        print(f\"\\nAugmented Dickey-Fuller Test:\")\n",
    "        print(f\"  ADF Statistic: {adf_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"  Critical Values: {adf_result[4]}\")\n",
    "        adf_stationary = adf_result[1] < alpha\n",
    "        print(f\"  Result: {'STATIONARY' if adf_stationary else 'NON-STATIONARY'}\")\n",
    "\n",
    "        # KPSS Test (H0: series is stationary)\n",
    "        kpss_result = kpss(series.dropna(), regression='ct', nlags='auto')\n",
    "        print(f\"\\nKPSS Test:\")\n",
    "        print(f\"  KPSS Statistic: {kpss_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
    "        print(f\"  Critical Values: {kpss_result[3]}\")\n",
    "        kpss_stationary = kpss_result[1] > alpha\n",
    "        print(f\"  Result: {'STATIONARY' if kpss_stationary else 'NON-STATIONARY'}\")\n",
    "\n",
    "        self.stationarity_results = {\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'adf_stationary': adf_stationary,\n",
    "            'kpss_statistic': kpss_result[0],\n",
    "            'kpss_pvalue': kpss_result[1],\n",
    "            'kpss_stationary': kpss_stationary,\n",
    "            'is_stationary': adf_stationary and kpss_stationary\n",
    "        }\n",
    "\n",
    "        return self.stationarity_results['is_stationary']\n",
    "\n",
    "    def make_stationary(self, series):\n",
    "        \"\"\"\n",
    "        Apply transformations to make series stationary\n",
    "        \"\"\"\n",
    "        original_series = series.copy()\n",
    "\n",
    "        if not self.test_stationarity(series):\n",
    "            print(\"\\n=== MAKING SERIES STATIONARY ===\")\n",
    "\n",
    "            # Try log transformation first (variance stabilization)\n",
    "            if (series > 0).all():\n",
    "                series_log = np.log(series)\n",
    "                if self.test_stationarity(series_log):\n",
    "                    print(\"Log transformation sufficient\")\n",
    "                    self.transformation = 'log'\n",
    "                    return series_log\n",
    "\n",
    "            # Apply first differencing\n",
    "            series_diff = series.diff().dropna()\n",
    "            if self.test_stationarity(series_diff):\n",
    "                print(\"First differencing sufficient\")\n",
    "                self.differencing_order = 1\n",
    "                self.transformation = 'diff'\n",
    "                return series_diff\n",
    "\n",
    "            # Apply second differencing if needed\n",
    "            series_diff2 = series_diff.diff().dropna()\n",
    "            if self.test_stationarity(series_diff2):\n",
    "                print(\"Second differencing required\")\n",
    "                self.differencing_order = 2\n",
    "                self.transformation = 'diff2'\n",
    "                return series_diff2\n",
    "\n",
    "            print(\"Warning: Could not achieve stationarity. Proceeding with first difference.\")\n",
    "            self.differencing_order = 1\n",
    "            self.transformation = 'diff'\n",
    "            return series.diff().dropna()\n",
    "\n",
    "        print(\"Series is already stationary\")\n",
    "        self.transformation = 'none'\n",
    "        return series\n",
    "\n",
    "    def find_optimal_lags(self, series, method='acf', threshold=0.2):\n",
    "        \"\"\"\n",
    "        Determine optimal lag order using ACF/PACF\n",
    "        \"\"\"\n",
    "        print(\"\\n=== FINDING OPTIMAL LAGS ===\")\n",
    "\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        # ACF\n",
    "        acf_values = acf(series_clean, nlags=self.max_lags, fft=False)\n",
    "        significant_acf = np.where(np.abs(acf_values[1:]) > threshold)[0] + 1\n",
    "\n",
    "        # PACF\n",
    "        pacf_values = pacf(series_clean, nlags=self.max_lags, method='ols')\n",
    "        significant_pacf = np.where(np.abs(pacf_values[1:]) > threshold)[0] + 1\n",
    "\n",
    "        if method == 'acf':\n",
    "            self.optimal_lags = significant_acf[:10].tolist()  # Top 10 lags\n",
    "        elif method == 'pacf':\n",
    "            self.optimal_lags = significant_pacf[:10].tolist()\n",
    "        else:  # both\n",
    "            self.optimal_lags = sorted(list(set(significant_acf[:10].tolist() +\n",
    "                                                 significant_pacf[:10].tolist())))\n",
    "\n",
    "        if not self.optimal_lags:\n",
    "            self.optimal_lags = [1, 2, 3, 5, 7]  # Default lags\n",
    "\n",
    "        print(f\"Optimal lags: {self.optimal_lags}\")\n",
    "        return self.optimal_lags\n",
    "\n",
    "    def stl_decomposition(self, series):\n",
    "        \"\"\"\n",
    "        Perform STL decomposition\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STL DECOMPOSITION ===\")\n",
    "\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        if len(series_clean) < 2 * self.seasonal_period:\n",
    "            print(f\"Warning: Series too short for seasonal decomposition (need >= {2*self.seasonal_period})\")\n",
    "            return None\n",
    "\n",
    "        stl = STL(series_clean, seasonal=self.seasonal_period, robust=True)\n",
    "        result = stl.fit()\n",
    "\n",
    "        self.stl_components = {\n",
    "            'trend': result.trend,\n",
    "            'seasonal': result.seasonal,\n",
    "            'resid': result.resid\n",
    "        }\n",
    "\n",
    "        print(f\"Trend strength: {1 - result.resid.var() / (result.trend + result.resid).var():.4f}\")\n",
    "        print(f\"Seasonal strength: {1 - result.resid.var() / (result.seasonal + result.resid).var():.4f}\")\n",
    "\n",
    "        return self.stl_components\n",
    "\n",
    "    def test_heteroscedasticity(self, residuals, features):\n",
    "        \"\"\"\n",
    "        Test for heteroscedasticity using Breusch-Pagan and White tests\n",
    "        \"\"\"\n",
    "        print(\"\\n=== HETEROSCEDASTICITY TESTS ===\")\n",
    "\n",
    "        # Breusch-Pagan test\n",
    "        try:\n",
    "            bp_stat, bp_pval, _, _ = het_breuschpagan(residuals, features)\n",
    "            print(f\"\\nBreusch-Pagan Test:\")\n",
    "            print(f\"  LM Statistic: {bp_stat:.4f}\")\n",
    "            print(f\"  p-value: {bp_pval:.4f}\")\n",
    "            bp_hetero = bp_pval < 0.05\n",
    "            print(f\"  Result: {'HETEROSCEDASTIC' if bp_hetero else 'HOMOSCEDASTIC'}\")\n",
    "        except:\n",
    "            bp_hetero = True\n",
    "            bp_pval = 0.0\n",
    "            print(\"Breusch-Pagan test failed\")\n",
    "\n",
    "        # White test\n",
    "        try:\n",
    "            white_stat, white_pval, _, _ = het_white(residuals, features)\n",
    "            print(f\"\\nWhite Test:\")\n",
    "            print(f\"  LM Statistic: {white_stat:.4f}\")\n",
    "            print(f\"  p-value: {white_pval:.4f}\")\n",
    "            white_hetero = white_pval < 0.05\n",
    "            print(f\"  Result: {'HETEROSCEDASTIC' if white_hetero else 'HOMOSCEDASTIC'}\")\n",
    "        except:\n",
    "            white_hetero = True\n",
    "            white_pval = 0.0\n",
    "            print(\"White test failed\")\n",
    "\n",
    "        self.heteroscedasticity_results = {\n",
    "            'bp_pvalue': bp_pval,\n",
    "            'white_pvalue': white_pval,\n",
    "            'is_heteroscedastic': bp_hetero or white_hetero\n",
    "        }\n",
    "\n",
    "        return self.heteroscedasticity_results['is_heteroscedastic']\n",
    "\n",
    "    def create_features(self, series, include_volatility=True):\n",
    "        \"\"\"\n",
    "        Create feature matrix with lags, rolling statistics, and volatility features\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'target': series})\n",
    "\n",
    "        # Lag features\n",
    "        for lag in self.optimal_lags:\n",
    "            df[f'lag_{lag}'] = df['target'].shift(lag)\n",
    "\n",
    "        # Rolling statistics\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'rolling_mean_{window}'] = df['target'].rolling(window).mean()\n",
    "            df[f'rolling_std_{window}'] = df['target'].rolling(window).std()\n",
    "            df[f'rolling_min_{window}'] = df['target'].rolling(window).min()\n",
    "            df[f'rolling_max_{window}'] = df['target'].rolling(window).max()\n",
    "\n",
    "        # Volatility features (if enabled)\n",
    "        if include_volatility:\n",
    "            # Historical volatility at multiple horizons\n",
    "            for window in [5, 10, 20, 60]:\n",
    "                df[f'volatility_{window}'] = df['target'].rolling(window).std()\n",
    "\n",
    "            # Realized volatility (if returns)\n",
    "            df['realized_vol'] = df['target'].rolling(20).std() * np.sqrt(252)\n",
    "\n",
    "            # Volatility of volatility\n",
    "            df['vol_of_vol'] = df['volatility_20'].rolling(20).std()\n",
    "\n",
    "            # Volatility regime (high/low)\n",
    "            vol_median = df['volatility_20'].rolling(60).median()\n",
    "            df['high_vol_regime'] = (df['volatility_20'] > vol_median).astype(int)\n",
    "\n",
    "            # Range-based volatility proxy\n",
    "            df['range_vol'] = (df['rolling_max_20'] - df['rolling_min_20']) / df['rolling_mean_20']\n",
    "\n",
    "        # STL components (if available)\n",
    "        if self.stl_components is not None:\n",
    "            df['trend'] = self.stl_components['trend']\n",
    "            df['seasonal'] = self.stl_components['seasonal']\n",
    "\n",
    "        # Time features\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df['day_of_week'] = df.index.dayofweek\n",
    "            df['month'] = df.index.month\n",
    "            df['quarter'] = df.index.quarter\n",
    "            df['day_of_month'] = df.index.day\n",
    "\n",
    "        # Drop NaN rows\n",
    "        df = df.dropna()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_volatility_weights(self, series, window=20, method='inverse_variance'):\n",
    "        \"\"\"\n",
    "        Create sample weights based on volatility\n",
    "        \"\"\"\n",
    "        if method == 'inverse_variance':\n",
    "            rolling_var = series.rolling(window).var()\n",
    "            weights = 1 / (rolling_var + 1e-8)  # Add small constant to avoid division by zero\n",
    "        elif method == 'exponential':\n",
    "            half_life = window\n",
    "            weights = np.exp(-np.log(2) * np.arange(len(series))[::-1] / half_life)\n",
    "        else:\n",
    "            weights = np.ones(len(series))\n",
    "\n",
    "        weights = weights / weights.sum() * len(series)  # Normalize to sum to n\n",
    "        return weights\n",
    "\n",
    "    def rolling_window_cv(self, data, target_col='target',\n",
    "                          initial_train_size=None, step_size=1,\n",
    "                          use_quantile=False, quantiles=[0.1, 0.5, 0.9]):\n",
    "        \"\"\"\n",
    "        Perform rolling window cross-validation\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : DataFrame\n",
    "            Feature matrix with target column\n",
    "        target_col : str\n",
    "            Name of target column\n",
    "        initial_train_size : int or None\n",
    "            Initial training window size (if None, uses one seasonal period)\n",
    "        step_size : int\n",
    "            Number of periods to roll forward\n",
    "        use_quantile : bool\n",
    "            Whether to use quantile regression\n",
    "        quantiles : list\n",
    "            Quantiles to predict if use_quantile=True\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ROLLING WINDOW CROSS-VALIDATION ===\")\n",
    "\n",
    "        if initial_train_size is None:\n",
    "            initial_train_size = self.seasonal_period\n",
    "\n",
    "        feature_cols = [col for col in data.columns if col != target_col]\n",
    "        X = data[feature_cols].values\n",
    "        y = data[target_col].values\n",
    "\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        prediction_intervals = []\n",
    "        fold_metrics = []\n",
    "\n",
    "        n_folds = (len(data) - initial_train_size) // step_size\n",
    "\n",
    "        for fold in range(n_folds):\n",
    "            train_end = initial_train_size + fold * step_size\n",
    "            test_start = train_end\n",
    "            test_end = min(test_start + step_size, len(data))\n",
    "\n",
    "            if test_end >= len(data):\n",
    "                break\n",
    "\n",
    "            X_train, y_train = X[:train_end], y[:train_end]\n",
    "            X_test, y_test = X[test_start:test_end], y[test_start:test_end]\n",
    "\n",
    "            print(f\"\\nFold {fold + 1}/{n_folds}: Train size={len(X_train)}, Test size={len(X_test)}\")\n",
    "\n",
    "            # Create volatility-based sample weights\n",
    "            weights = self.create_volatility_weights(\n",
    "                pd.Series(y_train),\n",
    "                method='inverse_variance'\n",
    "            )\n",
    "\n",
    "            if use_quantile:\n",
    "                # Quantile regression approach\n",
    "                fold_predictions = {}\n",
    "\n",
    "                for q in quantiles:\n",
    "                    model = XGBRegressor(\n",
    "                        objective='reg:quantileerror',\n",
    "                        quantile_alpha=q,\n",
    "                        n_estimators=100,\n",
    "                        max_depth=4,\n",
    "                        learning_rate=0.05,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model.fit(X_train, y_train, sample_weight=weights, verbose=False)\n",
    "                    fold_predictions[q] = model.predict(X_test)\n",
    "\n",
    "                # Use median as point prediction\n",
    "                pred = fold_predictions[0.5]\n",
    "                lower = fold_predictions[quantiles[0]]\n",
    "                upper = fold_predictions[quantiles[-1]]\n",
    "\n",
    "                prediction_intervals.extend(list(zip(lower, upper)))\n",
    "\n",
    "            else:\n",
    "                # Two-model approach: Mean + Volatility\n",
    "\n",
    "                # Model 1: Mean prediction\n",
    "                mean_model = XGBRegressor(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                mean_model.fit(X_train, y_train, sample_weight=weights, verbose=False)\n",
    "                pred_mean = mean_model.predict(X_test)\n",
    "\n",
    "                # Model 2: Volatility prediction\n",
    "                train_residuals = y_train - mean_model.predict(X_train)\n",
    "                train_vol = pd.Series(train_residuals).rolling(20, min_periods=5).std().fillna(\n",
    "                    pd.Series(train_residuals).std()\n",
    "                ).values\n",
    "\n",
    "                vol_model = XGBRegressor(\n",
    "                    n_estimators=50,\n",
    "                    max_depth=3,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                vol_model.fit(X_train, train_vol, verbose=False)\n",
    "                pred_vol = vol_model.predict(X_test)\n",
    "\n",
    "                pred = pred_mean\n",
    "                lower = pred_mean - 1.96 * pred_vol\n",
    "                upper = pred_mean + 1.96 * pred_vol\n",
    "\n",
    "                prediction_intervals.extend(list(zip(lower, upper)))\n",
    "\n",
    "            predictions.extend(pred)\n",
    "            actuals.extend(y_test)\n",
    "\n",
    "            # Calculate fold metrics\n",
    "            rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "            mae = np.mean(np.abs(pred - y_test))\n",
    "\n",
    "            # Coverage of prediction intervals\n",
    "            in_interval = np.sum((y_test >= lower) & (y_test <= upper))\n",
    "            coverage = in_interval / len(y_test)\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'coverage': coverage\n",
    "            })\n",
    "\n",
    "            print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, Coverage: {coverage:.2%}\")\n",
    "\n",
    "        results = {\n",
    "            'predictions': np.array(predictions),\n",
    "            'actuals': np.array(actuals),\n",
    "            'prediction_intervals': prediction_intervals,\n",
    "            'fold_metrics': pd.DataFrame(fold_metrics)\n",
    "        }\n",
    "\n",
    "        # Overall metrics\n",
    "        overall_rmse = np.sqrt(np.mean((results['predictions'] - results['actuals']) ** 2))\n",
    "        overall_mae = np.mean(np.abs(results['predictions'] - results['actuals']))\n",
    "\n",
    "        print(f\"\\n=== OVERALL CROSS-VALIDATION RESULTS ===\")\n",
    "        print(f\"RMSE: {overall_rmse:.4f}\")\n",
    "        print(f\"MAE: {overall_mae:.4f}\")\n",
    "        print(f\"Mean Coverage: {results['fold_metrics']['coverage'].mean():.2%}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def fit(self, series, use_quantile=False):\n",
    "        \"\"\"\n",
    "        Complete fitting pipeline\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING HETEROSCEDASTICITY-AWARE XGBOOST PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Step 1: Make stationary\n",
    "        series_stationary = self.make_stationary(series)\n",
    "\n",
    "        # Step 2: Find optimal lags\n",
    "        self.find_optimal_lags(series_stationary)\n",
    "\n",
    "        # Step 3: STL decomposition\n",
    "        self.stl_decomposition(series_stationary)\n",
    "\n",
    "        # Step 4: Create features\n",
    "        features_df = self.create_features(series_stationary, include_volatility=True)\n",
    "\n",
    "        # Step 5: Test heteroscedasticity (on a preliminary model)\n",
    "        X_temp = features_df.drop('target', axis=1).values\n",
    "        y_temp = features_df['target'].values\n",
    "\n",
    "        temp_model = XGBRegressor(n_estimators=50, max_depth=3, random_state=42)\n",
    "        temp_model.fit(X_temp[:len(X_temp)//2], y_temp[:len(y_temp)//2], verbose=False)\n",
    "        temp_residuals = y_temp[:len(y_temp)//2] - temp_model.predict(X_temp[:len(X_temp)//2])\n",
    "\n",
    "        is_hetero = self.test_heteroscedasticity(temp_residuals, X_temp[:len(X_temp)//2])\n",
    "\n",
    "        if is_hetero:\n",
    "            print(\"\\n*** HETEROSCEDASTICITY DETECTED - Using volatility modeling ***\")\n",
    "\n",
    "        # Step 6: Rolling window cross-validation\n",
    "        cv_results = self.rolling_window_cv(\n",
    "            features_df,\n",
    "            use_quantile=use_quantile,\n",
    "            initial_train_size=self.seasonal_period\n",
    "        )\n",
    "\n",
    "        return cv_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic commodity price data with heteroscedasticity\n",
    "    np.random.seed(42)\n",
    "    n = 500\n",
    "    t = np.arange(n)\n",
    "\n",
    "    # Trend + Seasonality + Heteroscedastic noise\n",
    "    trend = 0.05 * t\n",
    "    seasonal = 10 * np.sin(2 * np.pi * t / 12)\n",
    "\n",
    "    # Heteroscedastic volatility (GARCH-like)\n",
    "    volatility = np.zeros(n)\n",
    "    volatility[0] = 1.0\n",
    "    for i in range(1, n):\n",
    "        volatility[i] = 0.1 + 0.85 * volatility[i-1] + 0.1 * np.random.randn()**2\n",
    "\n",
    "    noise = volatility * np.random.randn(n)\n",
    "    price = 100 + trend + seasonal + noise\n",
    "\n",
    "    # Create time series\n",
    "    dates = pd.date_range('2020-01-01', periods=n, freq='D')\n",
    "    series = pd.Series(price, index=dates, name='price')\n",
    "\n",
    "    print(\"Sample data shape:\", series.shape)\n",
    "    print(\"Sample data (first 5):\", series.head())\n",
    "\n",
    "    # Initialize and fit pipeline\n",
    "    pipeline = HeteroscedasticXGBoostPipeline(seasonal_period=12, max_lags=20)\n",
    "\n",
    "    # Option 1: Quantile regression (recommended for heteroscedasticity)\n",
    "    results_quantile = pipeline.fit(series, use_quantile=True)\n",
    "\n",
    "    # Option 2: Two-model approach (mean + volatility)\n",
    "    # results_two_model = pipeline.fit(series, use_quantile=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vo_u6oYhaBhY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 256283,
     "status": "ok",
     "timestamp": 1762136121664,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "FXdAoihJZCw",
    "outputId": "d9ec113a-8b2c-4ac0-d205-4922a843fa2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data shape: (500,)\n",
      "Sample data (first 5): 2020-01-01    98.6172\n",
      "2020-01-02   105.9527\n",
      "2020-01-03   110.5367\n",
      "2020-01-04   108.8454\n",
      "2020-01-05   109.4935\n",
      "Freq: D, Name: price, dtype: float64\n",
      "============================================================\n",
      "STARTING HETEROSCEDASTICITY-AWARE XGBOOST PIPELINE\n",
      "============================================================\n",
      "\n",
      "=== STATIONARITY TESTS ===\n",
      "\n",
      "Augmented Dickey-Fuller Test:\n",
      "  ADF Statistic: -0.5331\n",
      "  p-value: 0.8854\n",
      "  Critical Values: {'1%': np.float64(-3.4438771098680196), '5%': np.float64(-2.867505393939065), '10%': np.float64(-2.569947324764179)}\n",
      "  Result: NON-STATIONARY\n",
      "\n",
      "KPSS Test:\n",
      "  KPSS Statistic: 0.0220\n",
      "  p-value: 0.1000\n",
      "  Critical Values: {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216}\n",
      "  Result: STATIONARY\n",
      "\n",
      "=== MAKING SERIES STATIONARY ===\n",
      "\n",
      "=== STATIONARITY TESTS ===\n",
      "\n",
      "Augmented Dickey-Fuller Test:\n",
      "  ADF Statistic: -0.9713\n",
      "  p-value: 0.7636\n",
      "  Critical Values: {'1%': np.float64(-3.4438771098680196), '5%': np.float64(-2.867505393939065), '10%': np.float64(-2.569947324764179)}\n",
      "  Result: NON-STATIONARY\n",
      "\n",
      "KPSS Test:\n",
      "  KPSS Statistic: 0.0413\n",
      "  p-value: 0.1000\n",
      "  Critical Values: {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216}\n",
      "  Result: STATIONARY\n",
      "\n",
      "=== STATIONARITY TESTS ===\n",
      "\n",
      "Augmented Dickey-Fuller Test:\n",
      "  ADF Statistic: -15.3803\n",
      "  p-value: 0.0000\n",
      "  Critical Values: {'1%': np.float64(-3.4438771098680196), '5%': np.float64(-2.867505393939065), '10%': np.float64(-2.569947324764179)}\n",
      "  Result: STATIONARY\n",
      "\n",
      "KPSS Test:\n",
      "  KPSS Statistic: 0.0097\n",
      "  p-value: 0.1000\n",
      "  Critical Values: {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216}\n",
      "  Result: STATIONARY\n",
      "First differencing sufficient\n",
      "\n",
      "=== FINDING OPTIMAL LAGS ===\n",
      "Optimal lags: [1, 2, 4, 5, 6, 7, 8, 10, 11, 12]\n",
      "\n",
      "=== STL DECOMPOSITION ===\n",
      "Using seasonal parameter: 13\n",
      "Trend strength: 0.5211\n",
      "Seasonal strength: 0.0797\n",
      "\n",
      "=== HETEROSCEDASTICITY TESTS ===\n",
      "Breusch-Pagan test failed\n",
      "White test failed\n",
      "\n",
      "*** HETEROSCEDASTICITY DETECTED - Using volatility modeling ***\n",
      "\n",
      "=== ROLLING WINDOW CROSS-VALIDATION ===\n",
      "\n",
      "Fold 1/428: Train size=12, Test size=1\n",
      "  RMSE: 2.5451, MAE: 2.5451, Coverage: 0.00%\n",
      "\n",
      "Fold 2/428: Train size=13, Test size=1\n",
      "  RMSE: 1.3676, MAE: 1.3676, Coverage: 100.00%\n",
      "\n",
      "Fold 3/428: Train size=14, Test size=1\n",
      "  RMSE: 0.6883, MAE: 0.6883, Coverage: 100.00%\n",
      "\n",
      "Fold 4/428: Train size=15, Test size=1\n",
      "  RMSE: 1.3449, MAE: 1.3449, Coverage: 100.00%\n",
      "\n",
      "Fold 5/428: Train size=16, Test size=1\n",
      "  RMSE: 5.9838, MAE: 5.9838, Coverage: 0.00%\n",
      "\n",
      "Fold 6/428: Train size=17, Test size=1\n",
      "  RMSE: 3.9756, MAE: 3.9756, Coverage: 100.00%\n",
      "\n",
      "Fold 7/428: Train size=18, Test size=1\n",
      "  RMSE: 0.5888, MAE: 0.5888, Coverage: 100.00%\n",
      "\n",
      "Fold 8/428: Train size=19, Test size=1\n",
      "  RMSE: 4.4903, MAE: 4.4903, Coverage: 0.00%\n",
      "\n",
      "Fold 9/428: Train size=20, Test size=1\n",
      "  RMSE: 5.9007, MAE: 5.9007, Coverage: 100.00%\n",
      "\n",
      "Fold 10/428: Train size=21, Test size=1\n",
      "  RMSE: 1.1232, MAE: 1.1232, Coverage: 0.00%\n",
      "\n",
      "Fold 11/428: Train size=22, Test size=1\n",
      "  RMSE: 3.5822, MAE: 3.5822, Coverage: 100.00%\n",
      "\n",
      "Fold 12/428: Train size=23, Test size=1\n",
      "  RMSE: 4.2618, MAE: 4.2618, Coverage: 0.00%\n",
      "\n",
      "Fold 13/428: Train size=24, Test size=1\n",
      "  RMSE: 3.9682, MAE: 3.9682, Coverage: 0.00%\n",
      "\n",
      "Fold 14/428: Train size=25, Test size=1\n",
      "  RMSE: 3.0279, MAE: 3.0279, Coverage: 100.00%\n",
      "\n",
      "Fold 15/428: Train size=26, Test size=1\n",
      "  RMSE: 0.2678, MAE: 0.2678, Coverage: 100.00%\n",
      "\n",
      "Fold 16/428: Train size=27, Test size=1\n",
      "  RMSE: 0.1158, MAE: 0.1158, Coverage: 100.00%\n",
      "\n",
      "Fold 17/428: Train size=28, Test size=1\n",
      "  RMSE: 1.6269, MAE: 1.6269, Coverage: 100.00%\n",
      "\n",
      "Fold 18/428: Train size=29, Test size=1\n",
      "  RMSE: 1.7181, MAE: 1.7181, Coverage: 100.00%\n",
      "\n",
      "Fold 19/428: Train size=30, Test size=1\n",
      "  RMSE: 2.3730, MAE: 2.3730, Coverage: 0.00%\n",
      "\n",
      "Fold 20/428: Train size=31, Test size=1\n",
      "  RMSE: 1.3681, MAE: 1.3681, Coverage: 100.00%\n",
      "\n",
      "Fold 21/428: Train size=32, Test size=1\n",
      "  RMSE: 2.1948, MAE: 2.1948, Coverage: 0.00%\n",
      "\n",
      "Fold 22/428: Train size=33, Test size=1\n",
      "  RMSE: 1.8597, MAE: 1.8597, Coverage: 100.00%\n",
      "\n",
      "Fold 23/428: Train size=34, Test size=1\n",
      "  RMSE: 0.7283, MAE: 0.7283, Coverage: 100.00%\n",
      "\n",
      "Fold 24/428: Train size=35, Test size=1\n",
      "  RMSE: 0.6475, MAE: 0.6475, Coverage: 100.00%\n",
      "\n",
      "Fold 25/428: Train size=36, Test size=1\n",
      "  RMSE: 0.4752, MAE: 0.4752, Coverage: 100.00%\n",
      "\n",
      "Fold 26/428: Train size=37, Test size=1\n",
      "  RMSE: 1.3550, MAE: 1.3550, Coverage: 0.00%\n",
      "\n",
      "Fold 27/428: Train size=38, Test size=1\n",
      "  RMSE: 0.5075, MAE: 0.5075, Coverage: 100.00%\n",
      "\n",
      "Fold 28/428: Train size=39, Test size=1\n",
      "  RMSE: 0.3669, MAE: 0.3669, Coverage: 100.00%\n",
      "\n",
      "Fold 29/428: Train size=40, Test size=1\n",
      "  RMSE: 1.2980, MAE: 1.2980, Coverage: 100.00%\n",
      "\n",
      "Fold 30/428: Train size=41, Test size=1\n",
      "  RMSE: 0.7646, MAE: 0.7646, Coverage: 0.00%\n",
      "\n",
      "Fold 31/428: Train size=42, Test size=1\n",
      "  RMSE: 2.6466, MAE: 2.6466, Coverage: 0.00%\n",
      "\n",
      "Fold 32/428: Train size=43, Test size=1\n",
      "  RMSE: 1.1591, MAE: 1.1591, Coverage: 100.00%\n",
      "\n",
      "Fold 33/428: Train size=44, Test size=1\n",
      "  RMSE: 0.6290, MAE: 0.6290, Coverage: 0.00%\n",
      "\n",
      "Fold 34/428: Train size=45, Test size=1\n",
      "  RMSE: 1.1865, MAE: 1.1865, Coverage: 0.00%\n",
      "\n",
      "Fold 35/428: Train size=46, Test size=1\n",
      "  RMSE: 1.2789, MAE: 1.2789, Coverage: 0.00%\n",
      "\n",
      "Fold 36/428: Train size=47, Test size=1\n",
      "  RMSE: 2.9088, MAE: 2.9088, Coverage: 100.00%\n",
      "\n",
      "Fold 37/428: Train size=48, Test size=1\n",
      "  RMSE: 1.1831, MAE: 1.1831, Coverage: 0.00%\n",
      "\n",
      "Fold 38/428: Train size=49, Test size=1\n",
      "  RMSE: 0.3482, MAE: 0.3482, Coverage: 100.00%\n",
      "\n",
      "Fold 39/428: Train size=50, Test size=1\n",
      "  RMSE: 3.9042, MAE: 3.9042, Coverage: 0.00%\n",
      "\n",
      "Fold 40/428: Train size=51, Test size=1\n",
      "  RMSE: 2.3014, MAE: 2.3014, Coverage: 100.00%\n",
      "\n",
      "Fold 41/428: Train size=52, Test size=1\n",
      "  RMSE: 0.0401, MAE: 0.0401, Coverage: 0.00%\n",
      "\n",
      "Fold 42/428: Train size=53, Test size=1\n",
      "  RMSE: 0.7752, MAE: 0.7752, Coverage: 0.00%\n",
      "\n",
      "Fold 43/428: Train size=54, Test size=1\n",
      "  RMSE: 2.2688, MAE: 2.2688, Coverage: 0.00%\n",
      "\n",
      "Fold 44/428: Train size=55, Test size=1\n",
      "  RMSE: 4.1416, MAE: 4.1416, Coverage: 100.00%\n",
      "\n",
      "Fold 45/428: Train size=56, Test size=1\n",
      "  RMSE: 3.2853, MAE: 3.2853, Coverage: 0.00%\n",
      "\n",
      "Fold 46/428: Train size=57, Test size=1\n",
      "  RMSE: 0.8467, MAE: 0.8467, Coverage: 100.00%\n",
      "\n",
      "Fold 47/428: Train size=58, Test size=1\n",
      "  RMSE: 0.1441, MAE: 0.1441, Coverage: 100.00%\n",
      "\n",
      "Fold 48/428: Train size=59, Test size=1\n",
      "  RMSE: 0.2216, MAE: 0.2216, Coverage: 100.00%\n",
      "\n",
      "Fold 49/428: Train size=60, Test size=1\n",
      "  RMSE: 2.1787, MAE: 2.1787, Coverage: 0.00%\n",
      "\n",
      "Fold 50/428: Train size=61, Test size=1\n",
      "  RMSE: 0.0840, MAE: 0.0840, Coverage: 100.00%\n",
      "\n",
      "Fold 51/428: Train size=62, Test size=1\n",
      "  RMSE: 1.8165, MAE: 1.8165, Coverage: 0.00%\n",
      "\n",
      "Fold 52/428: Train size=63, Test size=1\n",
      "  RMSE: 1.6641, MAE: 1.6641, Coverage: 0.00%\n",
      "\n",
      "Fold 53/428: Train size=64, Test size=1\n",
      "  RMSE: 0.0326, MAE: 0.0326, Coverage: 100.00%\n",
      "\n",
      "Fold 54/428: Train size=65, Test size=1\n",
      "  RMSE: 0.0779, MAE: 0.0779, Coverage: 100.00%\n",
      "\n",
      "Fold 55/428: Train size=66, Test size=1\n",
      "  RMSE: 2.6701, MAE: 2.6701, Coverage: 0.00%\n",
      "\n",
      "Fold 56/428: Train size=67, Test size=1\n",
      "  RMSE: 4.3342, MAE: 4.3342, Coverage: 0.00%\n",
      "\n",
      "Fold 57/428: Train size=68, Test size=1\n",
      "  RMSE: 2.8958, MAE: 2.8958, Coverage: 100.00%\n",
      "\n",
      "Fold 58/428: Train size=69, Test size=1\n",
      "  RMSE: 2.2496, MAE: 2.2496, Coverage: 100.00%\n",
      "\n",
      "Fold 59/428: Train size=70, Test size=1\n",
      "  RMSE: 2.4996, MAE: 2.4996, Coverage: 0.00%\n",
      "\n",
      "Fold 60/428: Train size=71, Test size=1\n",
      "  RMSE: 2.6619, MAE: 2.6619, Coverage: 100.00%\n",
      "\n",
      "Fold 61/428: Train size=72, Test size=1\n",
      "  RMSE: 6.4355, MAE: 6.4355, Coverage: 0.00%\n",
      "\n",
      "Fold 62/428: Train size=73, Test size=1\n",
      "  RMSE: 1.2696, MAE: 1.2696, Coverage: 100.00%\n",
      "\n",
      "Fold 63/428: Train size=74, Test size=1\n",
      "  RMSE: 0.3216, MAE: 0.3216, Coverage: 100.00%\n",
      "\n",
      "Fold 64/428: Train size=75, Test size=1\n",
      "  RMSE: 1.4336, MAE: 1.4336, Coverage: 100.00%\n",
      "\n",
      "Fold 65/428: Train size=76, Test size=1\n",
      "  RMSE: 0.1620, MAE: 0.1620, Coverage: 100.00%\n",
      "\n",
      "Fold 66/428: Train size=77, Test size=1\n",
      "  RMSE: 0.2174, MAE: 0.2174, Coverage: 100.00%\n",
      "\n",
      "Fold 67/428: Train size=78, Test size=1\n",
      "  RMSE: 0.4944, MAE: 0.4944, Coverage: 0.00%\n",
      "\n",
      "Fold 68/428: Train size=79, Test size=1\n",
      "  RMSE: 2.6363, MAE: 2.6363, Coverage: 0.00%\n",
      "\n",
      "Fold 69/428: Train size=80, Test size=1\n",
      "  RMSE: 0.0740, MAE: 0.0740, Coverage: 0.00%\n",
      "\n",
      "Fold 70/428: Train size=81, Test size=1\n",
      "  RMSE: 1.2960, MAE: 1.2960, Coverage: 100.00%\n",
      "\n",
      "Fold 71/428: Train size=82, Test size=1\n",
      "  RMSE: 2.3694, MAE: 2.3694, Coverage: 0.00%\n",
      "\n",
      "Fold 72/428: Train size=83, Test size=1\n",
      "  RMSE: 1.8211, MAE: 1.8211, Coverage: 100.00%\n",
      "\n",
      "Fold 73/428: Train size=84, Test size=1\n",
      "  RMSE: 0.3575, MAE: 0.3575, Coverage: 100.00%\n",
      "\n",
      "Fold 74/428: Train size=85, Test size=1\n",
      "  RMSE: 2.4402, MAE: 2.4402, Coverage: 0.00%\n",
      "\n",
      "Fold 75/428: Train size=86, Test size=1\n",
      "  RMSE: 1.0285, MAE: 1.0285, Coverage: 100.00%\n",
      "\n",
      "Fold 76/428: Train size=87, Test size=1\n",
      "  RMSE: 2.7066, MAE: 2.7066, Coverage: 0.00%\n",
      "\n",
      "Fold 77/428: Train size=88, Test size=1\n",
      "  RMSE: 3.3984, MAE: 3.3984, Coverage: 0.00%\n",
      "\n",
      "Fold 78/428: Train size=89, Test size=1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 968\u001b[39m\n\u001b[32m    965\u001b[39m pipeline = HeteroscedasticXGBoostPipeline(seasonal_period=\u001b[32m12\u001b[39m, max_lags=\u001b[32m20\u001b[39m)\n\u001b[32m    967\u001b[39m \u001b[38;5;66;03m# Option 1: Quantile regression (recommended for heteroscedasticity)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m results_quantile = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_quantile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[38;5;66;03m# Option 2: Two-model approach (mean + volatility)\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# results_two_model = pipeline.fit(series, use_quantile=False)\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 928\u001b[39m, in \u001b[36mHeteroscedasticXGBoostPipeline.fit\u001b[39m\u001b[34m(self, series, use_quantile)\u001b[39m\n\u001b[32m    925\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m*** HETEROSCEDASTICITY DETECTED - Using volatility modeling ***\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# Step 6: Rolling window cross-validation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m cv_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrolling_window_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_quantile\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_quantile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseasonal_period\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 421\u001b[39m, in \u001b[36mHeteroscedasticXGBoostPipeline.rolling_window_cv\u001b[39m\u001b[34m(self, data, target_col, initial_train_size, step_size, use_quantile, quantiles)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m quantiles:\n\u001b[32m    411\u001b[39m     model = XGBRegressor(\n\u001b[32m    412\u001b[39m         objective=\u001b[33m'\u001b[39m\u001b[33mreg:quantileerror\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    413\u001b[39m         quantile_alpha=q,\n\u001b[32m   (...)\u001b[39m\u001b[32m    419\u001b[39m         random_state=\u001b[32m42\u001b[39m\n\u001b[32m    420\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m     fold_predictions[q] = model.predict(X_test)\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# Use median as point prediction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/ML/lib/python3.14/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/ML/lib/python3.14/site-packages/xgboost/sklearn.py:1365\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1363\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1380\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/ML/lib/python3.14/site-packages/xgboost/core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/ML/lib/python3.14/site-packages/xgboost/training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/ML/lib/python3.14/site-packages/xgboost/core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Heteroscedasticity-Aware XGBoost Pipeline for Commodity Price Prediction\n",
    "Includes: Stationarity testing, STL decomposition, volatility modeling, rolling window CV\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from scipy.stats import boxcox, jarque_bera\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class HeteroscedasticXGBoostPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for commodity price prediction with heteroscedasticity handling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seasonal_period=12, max_lags=20):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        seasonal_period : int\n",
    "            Expected seasonality period (12 for monthly, 252 for daily trading days)\n",
    "            Note: Will be adjusted to odd number >= 3 for STL decomposition\n",
    "        max_lags : int\n",
    "            Maximum number of lags to consider\n",
    "        \"\"\"\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.max_lags = max_lags\n",
    "        self.differencing_order = 0\n",
    "        self.transformation = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.optimal_lags = []\n",
    "\n",
    "        # Models\n",
    "        self.mean_model = None\n",
    "        self.vol_model = None\n",
    "        self.quantile_models = {}\n",
    "\n",
    "        # Results storage\n",
    "        self.stationarity_results = {}\n",
    "        self.heteroscedasticity_results = {}\n",
    "        self.stl_components = None\n",
    "\n",
    "    def test_stationarity(self, series, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Perform ADF and KPSS tests for stationarity\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STATIONARITY TESTS ===\")\n",
    "\n",
    "        # ADF Test (H0: unit root present - non-stationary)\n",
    "        adf_result = adfuller(series.dropna(), autolag='AIC')\n",
    "        print(f\"\\nAugmented Dickey-Fuller Test:\")\n",
    "        print(f\"  ADF Statistic: {adf_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"  Critical Values: {adf_result[4]}\")\n",
    "        adf_stationary = adf_result[1] < alpha\n",
    "        print(f\"  Result: {'STATIONARY' if adf_stationary else 'NON-STATIONARY'}\")\n",
    "\n",
    "        # KPSS Test (H0: series is stationary)\n",
    "        kpss_result = kpss(series.dropna(), regression='ct', nlags='auto')\n",
    "        print(f\"\\nKPSS Test:\")\n",
    "        print(f\"  KPSS Statistic: {kpss_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
    "        print(f\"  Critical Values: {kpss_result[3]}\")\n",
    "        kpss_stationary = kpss_result[1] > alpha\n",
    "        print(f\"  Result: {'STATIONARY' if kpss_stationary else 'NON-STATIONARY'}\")\n",
    "\n",
    "        self.stationarity_results = {\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'adf_stationary': adf_stationary,\n",
    "            'kpss_statistic': kpss_result[0],\n",
    "            'kpss_pvalue': kpss_result[1],\n",
    "            'kpss_stationary': kpss_stationary,\n",
    "            'is_stationary': adf_stationary and kpss_stationary\n",
    "        }\n",
    "\n",
    "        return self.stationarity_results['is_stationary']\n",
    "\n",
    "    def make_stationary(self, series):\n",
    "        \"\"\"\n",
    "        Apply transformations to make series stationary\n",
    "        \"\"\"\n",
    "        original_series = series.copy()\n",
    "\n",
    "        if not self.test_stationarity(series):\n",
    "            print(\"\\n=== MAKING SERIES STATIONARY ===\")\n",
    "\n",
    "            # Try log transformation first (variance stabilization)\n",
    "            if (series > 0).all():\n",
    "                series_log = np.log(series)\n",
    "                if self.test_stationarity(series_log):\n",
    "                    print(\"Log transformation sufficient\")\n",
    "                    self.transformation = 'log'\n",
    "                    return series_log\n",
    "\n",
    "            # Apply first differencing\n",
    "            series_diff = series.diff().dropna()\n",
    "            if self.test_stationarity(series_diff):\n",
    "                print(\"First differencing sufficient\")\n",
    "                self.differencing_order = 1\n",
    "                self.transformation = 'diff'\n",
    "                return series_diff\n",
    "\n",
    "            # Apply second differencing if needed\n",
    "            series_diff2 = series_diff.diff().dropna()\n",
    "            if self.test_stationarity(series_diff2):\n",
    "                print(\"Second differencing required\")\n",
    "                self.differencing_order = 2\n",
    "                self.transformation = 'diff2'\n",
    "                return series_diff2\n",
    "\n",
    "            print(\"Warning: Could not achieve stationarity. Proceeding with first difference.\")\n",
    "            self.differencing_order = 1\n",
    "            self.transformation = 'diff'\n",
    "            return series.diff().dropna()\n",
    "\n",
    "        print(\"Series is already stationary\")\n",
    "        self.transformation = 'none'\n",
    "        return series\n",
    "\n",
    "    def find_optimal_lags(self, series, method='acf', threshold=0.2):\n",
    "        \"\"\"\n",
    "        Determine optimal lag order using ACF/PACF\n",
    "        \"\"\"\n",
    "        print(\"\\n=== FINDING OPTIMAL LAGS ===\")\n",
    "\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        # ACF\n",
    "        acf_values = acf(series_clean, nlags=self.max_lags, fft=False)\n",
    "        significant_acf = np.where(np.abs(acf_values[1:]) > threshold)[0] + 1\n",
    "\n",
    "        # PACF\n",
    "        pacf_values = pacf(series_clean, nlags=self.max_lags, method='ols')\n",
    "        significant_pacf = np.where(np.abs(pacf_values[1:]) > threshold)[0] + 1\n",
    "\n",
    "        if method == 'acf':\n",
    "            self.optimal_lags = significant_acf[:10].tolist()  # Top 10 lags\n",
    "        elif method == 'pacf':\n",
    "            self.optimal_lags = significant_pacf[:10].tolist()\n",
    "        else:  # both\n",
    "            self.optimal_lags = sorted(list(set(significant_acf[:10].tolist() +\n",
    "                                                 significant_pacf[:10].tolist())))\n",
    "\n",
    "        if not self.optimal_lags:\n",
    "            self.optimal_lags = [1, 2, 3, 5, 7]  # Default lags\n",
    "\n",
    "        print(f\"Optimal lags: {self.optimal_lags}\")\n",
    "        return self.optimal_lags\n",
    "\n",
    "    def stl_decomposition(self, series):\n",
    "        \"\"\"\n",
    "        Perform STL decomposition\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STL DECOMPOSITION ===\")\n",
    "\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        if len(series_clean) < 2 * self.seasonal_period:\n",
    "            print(f\"Warning: Series too short for seasonal decomposition (need >= {2*self.seasonal_period})\")\n",
    "            return None\n",
    "\n",
    "        # Ensure seasonal parameter is odd and >= 3\n",
    "        seasonal_param = self.seasonal_period\n",
    "        if seasonal_param % 2 == 0:\n",
    "            seasonal_param += 1\n",
    "        if seasonal_param < 3:\n",
    "            seasonal_param = 3\n",
    "\n",
    "        print(f\"Using seasonal parameter: {seasonal_param}\")\n",
    "\n",
    "        try:\n",
    "            stl = STL(series_clean, seasonal=seasonal_param, robust=True)\n",
    "            result = stl.fit()\n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Proceeding without STL components\")\n",
    "            return None\n",
    "\n",
    "        self.stl_components = {\n",
    "            'trend': result.trend,\n",
    "            'seasonal': result.seasonal,\n",
    "            'resid': result.resid\n",
    "        }\n",
    "\n",
    "        print(f\"Trend strength: {1 - result.resid.var() / (result.trend + result.resid).var():.4f}\")\n",
    "        print(f\"Seasonal strength: {1 - result.resid.var() / (result.seasonal + result.resid).var():.4f}\")\n",
    "\n",
    "        return self.stl_components\n",
    "\n",
    "    def test_heteroscedasticity(self, residuals, features):\n",
    "        \"\"\"\n",
    "        Test for heteroscedasticity using Breusch-Pagan and White tests\n",
    "        \"\"\"\n",
    "        print(\"\\n=== HETEROSCEDASTICITY TESTS ===\")\n",
    "\n",
    "        # Breusch-Pagan test\n",
    "        try:\n",
    "            bp_stat, bp_pval, _, _ = het_breuschpagan(residuals, features)\n",
    "            print(f\"\\nBreusch-Pagan Test:\")\n",
    "            print(f\"  LM Statistic: {bp_stat:.4f}\")\n",
    "            print(f\"  p-value: {bp_pval:.4f}\")\n",
    "            bp_hetero = bp_pval < 0.05\n",
    "            print(f\"  Result: {'HETEROSCEDASTIC' if bp_hetero else 'HOMOSCEDASTIC'}\")\n",
    "        except:\n",
    "            bp_hetero = True\n",
    "            bp_pval = 0.0\n",
    "            print(\"Breusch-Pagan test failed\")\n",
    "\n",
    "        # White test\n",
    "        try:\n",
    "            white_stat, white_pval, _, _ = het_white(residuals, features)\n",
    "            print(f\"\\nWhite Test:\")\n",
    "            print(f\"  LM Statistic: {white_stat:.4f}\")\n",
    "            print(f\"  p-value: {white_pval:.4f}\")\n",
    "            white_hetero = white_pval < 0.05\n",
    "            print(f\"  Result: {'HETEROSCEDASTIC' if white_hetero else 'HOMOSCEDASTIC'}\")\n",
    "        except:\n",
    "            white_hetero = True\n",
    "            white_pval = 0.0\n",
    "            print(\"White test failed\")\n",
    "\n",
    "        self.heteroscedasticity_results = {\n",
    "            'bp_pvalue': bp_pval,\n",
    "            'white_pvalue': white_pval,\n",
    "            'is_heteroscedastic': bp_hetero or white_hetero\n",
    "        }\n",
    "\n",
    "        return self.heteroscedasticity_results['is_heteroscedastic']\n",
    "\n",
    "    def create_features(self, series, include_volatility=True):\n",
    "        \"\"\"\n",
    "        Create feature matrix with lags, rolling statistics, and volatility features\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'target': series})\n",
    "\n",
    "        # Lag features\n",
    "        for lag in self.optimal_lags:\n",
    "            df[f'lag_{lag}'] = df['target'].shift(lag)\n",
    "\n",
    "        # Rolling statistics\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'rolling_mean_{window}'] = df['target'].rolling(window).mean()\n",
    "            df[f'rolling_std_{window}'] = df['target'].rolling(window).std()\n",
    "            df[f'rolling_min_{window}'] = df['target'].rolling(window).min()\n",
    "            df[f'rolling_max_{window}'] = df['target'].rolling(window).max()\n",
    "\n",
    "        # Volatility features (if enabled)\n",
    "        if include_volatility:\n",
    "            # Historical volatility at multiple horizons\n",
    "            for window in [5, 10, 20, 60]:\n",
    "                df[f'volatility_{window}'] = df['target'].rolling(window).std()\n",
    "\n",
    "            # Realized volatility (if returns)\n",
    "            df['realized_vol'] = df['target'].rolling(20).std() * np.sqrt(252)\n",
    "\n",
    "            # Volatility of volatility\n",
    "            df['vol_of_vol'] = df['volatility_20'].rolling(20).std()\n",
    "\n",
    "            # Volatility regime (high/low)\n",
    "            vol_median = df['volatility_20'].rolling(60).median()\n",
    "            df['high_vol_regime'] = (df['volatility_20'] > vol_median).astype(int)\n",
    "\n",
    "            # Range-based volatility proxy\n",
    "            df['range_vol'] = (df['rolling_max_20'] - df['rolling_min_20']) / df['rolling_mean_20']\n",
    "\n",
    "        # STL components (if available)\n",
    "        if self.stl_components is not None:\n",
    "            df['trend'] = self.stl_components['trend']\n",
    "            df['seasonal'] = self.stl_components['seasonal']\n",
    "\n",
    "        # Time features\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df['day_of_week'] = df.index.dayofweek\n",
    "            df['month'] = df.index.month\n",
    "            df['quarter'] = df.index.quarter\n",
    "            df['day_of_month'] = df.index.day\n",
    "\n",
    "        # Drop NaN rows\n",
    "        df = df.dropna()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_volatility_weights(self, series, window=20, method='inverse_variance'):\n",
    "        \"\"\"\n",
    "        Create sample weights based on volatility\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if pandas Series\n",
    "        if isinstance(series, pd.Series):\n",
    "            values = series.values\n",
    "        else:\n",
    "            values = np.array(series)\n",
    "\n",
    "        if method == 'inverse_variance':\n",
    "            # Calculate rolling variance manually to avoid pandas issues\n",
    "            weights = np.ones(len(values))\n",
    "            for i in range(window, len(values)):\n",
    "                window_data = values[i-window:i]\n",
    "                var = np.var(window_data)\n",
    "                if var > 1e-10:  # Only if variance is meaningful\n",
    "                    weights[i] = 1.0 / var\n",
    "                else:\n",
    "                    weights[i] = 1.0\n",
    "\n",
    "            # Fill first window values with mean weight\n",
    "            mean_weight = np.mean(weights[window:])\n",
    "            weights[:window] = mean_weight\n",
    "\n",
    "        elif method == 'exponential':\n",
    "            half_life = window\n",
    "            weights = np.exp(-np.log(2) * np.arange(len(values))[::-1] / half_life)\n",
    "        else:\n",
    "            weights = np.ones(len(values))\n",
    "\n",
    "        # Ensure all weights are strictly positive\n",
    "        weights = np.abs(weights)  # Remove any negative\n",
    "        weights = np.where(weights < 1e-8, 1e-8, weights)  # Floor at small positive\n",
    "        weights = np.where(np.isnan(weights), 1.0, weights)  # Replace NaN\n",
    "        weights = np.where(np.isinf(weights), 1.0, weights)  # Replace Inf\n",
    "\n",
    "        # Normalize to sum to n (standard for XGBoost)\n",
    "        weights = weights / np.sum(weights) * len(weights)\n",
    "\n",
    "        # Final safety check\n",
    "        assert np.all(weights > 0), \"All weights must be positive\"\n",
    "        assert np.all(np.isfinite(weights)), \"All weights must be finite\"\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def rolling_window_cv(self, data, target_col='target',\n",
    "                          initial_train_size=None, step_size=1,\n",
    "                          use_quantile=False, quantiles=[0.1, 0.5, 0.9]):\n",
    "        \"\"\"\n",
    "        Perform rolling window cross-validation\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : DataFrame\n",
    "            Feature matrix with target column\n",
    "        target_col : str\n",
    "            Name of target column\n",
    "        initial_train_size : int or None\n",
    "            Initial training window size (if None, uses one seasonal period)\n",
    "        step_size : int\n",
    "            Number of periods to roll forward\n",
    "        use_quantile : bool\n",
    "            Whether to use quantile regression\n",
    "        quantiles : list\n",
    "            Quantiles to predict if use_quantile=True\n",
    "        \"\"\"\n",
    "        print(\"\\n=== ROLLING WINDOW CROSS-VALIDATION ===\")\n",
    "\n",
    "        if initial_train_size is None:\n",
    "            initial_train_size = self.seasonal_period\n",
    "\n",
    "        feature_cols = [col for col in data.columns if col != target_col]\n",
    "        X = data[feature_cols].values\n",
    "        y = data[target_col].values\n",
    "\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        prediction_intervals = []\n",
    "        fold_metrics = []\n",
    "\n",
    "        n_folds = (len(data) - initial_train_size) // step_size\n",
    "\n",
    "        for fold in range(n_folds):\n",
    "            train_end = initial_train_size + fold * step_size\n",
    "            test_start = train_end\n",
    "            test_end = min(test_start + step_size, len(data))\n",
    "\n",
    "            if test_end >= len(data):\n",
    "                break\n",
    "\n",
    "            X_train, y_train = X[:train_end], y[:train_end]\n",
    "            X_test, y_test = X[test_start:test_end], y[test_start:test_end]\n",
    "\n",
    "            print(f\"\\nFold {fold + 1}/{n_folds}: Train size={len(X_train)}, Test size={len(X_test)}\")\n",
    "\n",
    "            # Create volatility-based sample weights\n",
    "            try:\n",
    "                weights = self.create_volatility_weights(\n",
    "                    y_train,  # Pass numpy array directly\n",
    "                    window=min(20, len(y_train)//2),  # Adjust window for small samples\n",
    "                    method='inverse_variance'\n",
    "                )\n",
    "\n",
    "                # Validate weights\n",
    "                assert len(weights) == len(y_train), f\"Weight length mismatch: {len(weights)} vs {len(y_train)}\"\n",
    "                assert np.all(weights > 0), \"Some weights are non-positive\"\n",
    "                assert np.all(np.isfinite(weights)), \"Some weights are not finite\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Weight creation failed ({e}). Using uniform weights.\")\n",
    "                weights = np.ones(len(y_train))\n",
    "\n",
    "            if use_quantile:\n",
    "                # Quantile regression approach\n",
    "                fold_predictions = {}\n",
    "\n",
    "                for q in quantiles:\n",
    "                    model = XGBRegressor(\n",
    "                        objective='reg:quantileerror',\n",
    "                        quantile_alpha=q,\n",
    "                        n_estimators=100,\n",
    "                        max_depth=4,\n",
    "                        learning_rate=0.05,\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    model.fit(X_train, y_train, sample_weight=weights, verbose=False)\n",
    "                    fold_predictions[q] = model.predict(X_test)\n",
    "\n",
    "                # Use median as point prediction\n",
    "                pred = fold_predictions[0.5]\n",
    "                lower = fold_predictions[quantiles[0]]\n",
    "                upper = fold_predictions[quantiles[-1]]\n",
    "\n",
    "                prediction_intervals.extend(list(zip(lower, upper)))\n",
    "\n",
    "            else:\n",
    "                # Two-model approach: Mean + Volatility\n",
    "\n",
    "                # Model 1: Mean prediction\n",
    "                mean_model = XGBRegressor(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                mean_model.fit(X_train, y_train, sample_weight=weights, verbose=False)\n",
    "                pred_mean = mean_model.predict(X_test)\n",
    "\n",
    "                # Model 2: Volatility prediction\n",
    "                train_residuals = y_train - mean_model.predict(X_train)\n",
    "                train_vol = pd.Series(train_residuals).rolling(20, min_periods=5).std().fillna(\n",
    "                    pd.Series(train_residuals).std()\n",
    "                ).values\n",
    "\n",
    "                vol_model = XGBRegressor(\n",
    "                    n_estimators=50,\n",
    "                    max_depth=3,\n",
    "                    learning_rate=0.05,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42\n",
    "                )\n",
    "                vol_model.fit(X_train, train_vol, verbose=False)\n",
    "                pred_vol = vol_model.predict(X_test)\n",
    "\n",
    "                pred = pred_mean\n",
    "                lower = pred_mean - 1.96 * pred_vol\n",
    "                upper = pred_mean + 1.96 * pred_vol\n",
    "\n",
    "                prediction_intervals.extend(list(zip(lower, upper)))\n",
    "\n",
    "            predictions.extend(pred)\n",
    "            actuals.extend(y_test)\n",
    "\n",
    "            # Calculate fold metrics\n",
    "            rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "            mae = np.mean(np.abs(pred - y_test))\n",
    "\n",
    "            # Coverage of prediction intervals\n",
    "            in_interval = np.sum((y_test >= lower) & (y_test <= upper))\n",
    "            coverage = in_interval / len(y_test)\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'coverage': coverage\n",
    "            })\n",
    "\n",
    "            print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, Coverage: {coverage:.2%}\")\n",
    "\n",
    "        results = {\n",
    "            'predictions': np.array(predictions),\n",
    "            'actuals': np.array(actuals),\n",
    "            'prediction_intervals': prediction_intervals,\n",
    "            'fold_metrics': pd.DataFrame(fold_metrics)\n",
    "        }\n",
    "\n",
    "        # Overall metrics\n",
    "        overall_rmse = np.sqrt(np.mean((results['predictions'] - results['actuals']) ** 2))\n",
    "        overall_mae = np.mean(np.abs(results['predictions'] - results['actuals']))\n",
    "\n",
    "        print(f\"\\n=== OVERALL CROSS-VALIDATION RESULTS ===\")\n",
    "        print(f\"RMSE: {overall_rmse:.4f}\")\n",
    "        print(f\"MAE: {overall_mae:.4f}\")\n",
    "        print(f\"Mean Coverage: {results['fold_metrics']['coverage'].mean():.2%}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_diagnostics(self, original_series, cv_results, save_path=None):\n",
    "        \"\"\"\n",
    "        Comprehensive diagnostic plots for model evaluation\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        original_series : pd.Series\n",
    "            Original time series data\n",
    "        cv_results : dict\n",
    "            Results from rolling_window_cv\n",
    "        save_path : str, optional\n",
    "            Path to save figures\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        gs = fig.add_gridspec(5, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "        # 1. Original Series with Trend and Seasonality\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1.plot(original_series.index, original_series.values,\n",
    "                 label='Original', alpha=0.7, linewidth=1)\n",
    "        if self.stl_components is not None:\n",
    "            ax1.plot(self.stl_components['trend'].index,\n",
    "                    self.stl_components['trend'].values,\n",
    "                    label='Trend', linewidth=2, color='red')\n",
    "        ax1.set_title('Original Time Series with Trend', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('Price')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. STL Decomposition Components\n",
    "        if self.stl_components is not None:\n",
    "            ax2 = fig.add_subplot(gs[1, 0])\n",
    "            ax2.plot(self.stl_components['trend'].index,\n",
    "                    self.stl_components['trend'].values, color='red')\n",
    "            ax2.set_title('Trend Component', fontsize=12, fontweight='bold')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            ax3 = fig.add_subplot(gs[1, 1])\n",
    "            ax3.plot(self.stl_components['seasonal'].index,\n",
    "                    self.stl_components['seasonal'].values, color='green')\n",
    "            ax3.set_title('Seasonal Component', fontsize=12, fontweight='bold')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "\n",
    "            ax4 = fig.add_subplot(gs[1, 2])\n",
    "            ax4.plot(self.stl_components['resid'].index,\n",
    "                    self.stl_components['resid'].values, color='purple', alpha=0.6)\n",
    "            ax4.set_title('Residual Component', fontsize=12, fontweight='bold')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Predictions vs Actuals\n",
    "        ax5 = fig.add_subplot(gs[2, :])\n",
    "        test_indices = range(len(cv_results['actuals']))\n",
    "        ax5.plot(test_indices, cv_results['actuals'],\n",
    "                label='Actual', color='blue', linewidth=2, alpha=0.7)\n",
    "        ax5.plot(test_indices, cv_results['predictions'],\n",
    "                label='Predicted', color='red', linewidth=2, alpha=0.7)\n",
    "\n",
    "        # Add prediction intervals\n",
    "        if cv_results['prediction_intervals']:\n",
    "            intervals = np.array(cv_results['prediction_intervals'])\n",
    "            ax5.fill_between(test_indices, intervals[:, 0], intervals[:, 1],\n",
    "                           alpha=0.2, color='red', label='95% Prediction Interval')\n",
    "\n",
    "        ax5.set_title('Predictions vs Actuals (Cross-Validation)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax5.set_xlabel('Test Sample Index')\n",
    "        ax5.set_ylabel('Value')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Residuals Analysis\n",
    "        residuals = cv_results['predictions'] - cv_results['actuals']\n",
    "\n",
    "        ax6 = fig.add_subplot(gs[3, 0])\n",
    "        ax6.scatter(cv_results['predictions'], residuals, alpha=0.5, s=20)\n",
    "        ax6.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax6.set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "        ax6.set_xlabel('Predicted Values')\n",
    "        ax6.set_ylabel('Residuals')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add LOESS smooth to detect patterns\n",
    "        from scipy.signal import savgol_filter\n",
    "        if len(residuals) > 50:\n",
    "            sorted_idx = np.argsort(cv_results['predictions'])\n",
    "            smoothed = savgol_filter(residuals[sorted_idx],\n",
    "                                    min(51, len(residuals)//2*2-1), 3)\n",
    "            ax6.plot(cv_results['predictions'][sorted_idx], smoothed,\n",
    "                    color='orange', linewidth=2, label='Smoothed')\n",
    "            ax6.legend()\n",
    "\n",
    "        # 5. Residuals Histogram\n",
    "        ax7 = fig.add_subplot(gs[3, 1])\n",
    "        ax7.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax7.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax7.set_title('Residuals Distribution', fontsize=12, fontweight='bold')\n",
    "        ax7.set_xlabel('Residual Value')\n",
    "        ax7.set_ylabel('Frequency')\n",
    "\n",
    "        # Add normality statistics\n",
    "        from scipy.stats import shapiro\n",
    "        _, p_value = shapiro(residuals)\n",
    "        ax7.text(0.05, 0.95, f'Shapiro-Wilk p={p_value:.4f}',\n",
    "                transform=ax7.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6. Q-Q Plot\n",
    "        ax8 = fig.add_subplot(gs[3, 2])\n",
    "        from scipy.stats import probplot\n",
    "        probplot(residuals, dist=\"norm\", plot=ax8)\n",
    "        ax8.set_title('Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "\n",
    "        # 7. ACF of Residuals\n",
    "        ax9 = fig.add_subplot(gs[4, 0])\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plot_acf(residuals, lags=min(40, len(residuals)//2), ax=ax9, alpha=0.05)\n",
    "        ax9.set_title('ACF of Residuals', fontsize=12, fontweight='bold')\n",
    "        ax9.set_xlabel('Lag')\n",
    "\n",
    "        # 8. Rolling RMSE\n",
    "        ax10 = fig.add_subplot(gs[4, 1])\n",
    "        fold_metrics = cv_results['fold_metrics']\n",
    "        ax10.plot(fold_metrics['fold'], fold_metrics['rmse'],\n",
    "                 marker='o', linewidth=2, markersize=8)\n",
    "        ax10.set_title('RMSE by Fold', fontsize=12, fontweight='bold')\n",
    "        ax10.set_xlabel('Fold')\n",
    "        ax10.set_ylabel('RMSE')\n",
    "        ax10.grid(True, alpha=0.3)\n",
    "\n",
    "        # 9. Coverage Analysis\n",
    "        ax11 = fig.add_subplot(gs[4, 2])\n",
    "        ax11.plot(fold_metrics['fold'], fold_metrics['coverage'],\n",
    "                 marker='s', linewidth=2, markersize=8, color='green')\n",
    "        ax11.axhline(y=0.95, color='red', linestyle='--', linewidth=2,\n",
    "                    label='Target 95%')\n",
    "        ax11.set_title('Prediction Interval Coverage', fontsize=12, fontweight='bold')\n",
    "        ax11.set_xlabel('Fold')\n",
    "        ax11.set_ylabel('Coverage')\n",
    "        ax11.set_ylim([0, 1])\n",
    "        ax11.legend()\n",
    "        ax11.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('Comprehensive Model Diagnostics',\n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nDiagnostics saved to {save_path}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Additional separate plots\n",
    "        self._plot_volatility_analysis(original_series, cv_results)\n",
    "        self._plot_error_distribution(cv_results)\n",
    "\n",
    "    def _plot_volatility_analysis(self, series, cv_results):\n",
    "        \"\"\"\n",
    "        Detailed volatility analysis plots\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "        # 1. Rolling Volatility\n",
    "        returns = series.pct_change().dropna()\n",
    "        rolling_vol_5 = returns.rolling(5).std()\n",
    "        rolling_vol_20 = returns.rolling(20).std()\n",
    "        rolling_vol_60 = returns.rolling(60).std()\n",
    "\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.plot(rolling_vol_5.index, rolling_vol_5, label='5-period', alpha=0.6)\n",
    "        ax1.plot(rolling_vol_20.index, rolling_vol_20, label='20-period', linewidth=2)\n",
    "        ax1.plot(rolling_vol_60.index, rolling_vol_60, label='60-period', linewidth=2)\n",
    "        ax1.set_title('Rolling Volatility (Multiple Windows)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Volatility')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Volatility Clustering\n",
    "        ax2 = axes[0, 1]\n",
    "        abs_returns = np.abs(returns)\n",
    "        ax2.plot(abs_returns.index, abs_returns, alpha=0.5, linewidth=0.5)\n",
    "        ax2.plot(abs_returns.rolling(20).mean().index,\n",
    "                abs_returns.rolling(20).mean(),\n",
    "                color='red', linewidth=2, label='20-period MA')\n",
    "        ax2.set_title('Volatility Clustering (Absolute Returns)',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('|Returns|')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Squared Residuals ACF (ARCH test)\n",
    "        ax3 = axes[1, 0]\n",
    "        residuals = cv_results['predictions'] - cv_results['actuals']\n",
    "        squared_residuals = residuals ** 2\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plot_acf(squared_residuals, lags=min(40, len(squared_residuals)//2),\n",
    "                ax=ax3, alpha=0.05)\n",
    "        ax3.set_title('ACF of Squared Residuals (ARCH Effects)',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "\n",
    "        # 4. Volatility vs Prediction Error\n",
    "        ax4 = axes[1, 1]\n",
    "        # Estimate local volatility\n",
    "        window = 20\n",
    "        local_vol = pd.Series(cv_results['actuals']).rolling(window, min_periods=5).std()\n",
    "        abs_errors = np.abs(residuals)\n",
    "\n",
    "        ax4.scatter(local_vol, abs_errors, alpha=0.5, s=30)\n",
    "        ax4.set_title('Prediction Error vs Local Volatility',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Local Volatility (20-period)')\n",
    "        ax4.set_ylabel('Absolute Error')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add trend line\n",
    "        valid_idx = ~np.isnan(local_vol)\n",
    "        if valid_idx.sum() > 1:\n",
    "            z = np.polyfit(local_vol[valid_idx], abs_errors[valid_idx], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax4.plot(local_vol[valid_idx], p(local_vol[valid_idx]),\n",
    "                    \"r--\", linewidth=2, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "            ax4.legend()\n",
    "\n",
    "        plt.suptitle('Volatility Analysis', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_error_distribution(self, cv_results):\n",
    "        \"\"\"\n",
    "        Detailed error distribution analysis\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "        residuals = cv_results['predictions'] - cv_results['actuals']\n",
    "        abs_errors = np.abs(residuals)\n",
    "        pct_errors = 100 * residuals / cv_results['actuals']\n",
    "\n",
    "        # 1. Error over time\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.plot(residuals, alpha=0.7, linewidth=1)\n",
    "        ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax1.fill_between(range(len(residuals)), -2*residuals.std(),\n",
    "                        2*residuals.std(), alpha=0.2, color='red')\n",
    "        ax1.set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Residual')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Absolute Error Distribution\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.hist(abs_errors, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "        ax2.axvline(x=abs_errors.mean(), color='red', linestyle='--',\n",
    "                   linewidth=2, label=f'Mean: {abs_errors.mean():.4f}')\n",
    "        ax2.axvline(x=np.median(abs_errors), color='green', linestyle='--',\n",
    "                   linewidth=2, label=f'Median: {np.median(abs_errors):.4f}')\n",
    "        ax2.set_title('Absolute Error Distribution', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Absolute Error')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Percentage Error Distribution\n",
    "        ax3 = axes[0, 2]\n",
    "        ax3.hist(pct_errors, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "        ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax3.set_title('Percentage Error Distribution', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Error (%)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Error by Prediction Magnitude\n",
    "        ax4 = axes[1, 0]\n",
    "        pred_bins = pd.qcut(cv_results['predictions'], q=10, duplicates='drop')\n",
    "        error_by_bin = pd.DataFrame({\n",
    "            'pred_bin': pred_bins,\n",
    "            'abs_error': abs_errors\n",
    "        }).groupby('pred_bin')['abs_error'].agg(['mean', 'std'])\n",
    "\n",
    "        x_pos = range(len(error_by_bin))\n",
    "        ax4.bar(x_pos, error_by_bin['mean'], yerr=error_by_bin['std'],\n",
    "               alpha=0.7, capsize=5)\n",
    "        ax4.set_title('Error by Prediction Magnitude', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Prediction Bin (Deciles)')\n",
    "        ax4.set_ylabel('Mean Absolute Error')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # 5. Cumulative Error\n",
    "        ax5 = axes[1, 1]\n",
    "        cumulative_error = np.cumsum(residuals)\n",
    "        ax5.plot(cumulative_error, linewidth=2)\n",
    "        ax5.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax5.set_title('Cumulative Error (Bias Detection)', fontsize=12, fontweight='bold')\n",
    "        ax5.set_ylabel('Cumulative Error')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6. Error Metrics Summary\n",
    "        ax6 = axes[1, 2]\n",
    "        ax6.axis('off')\n",
    "\n",
    "        metrics_text = f\"\"\"\n",
    "        ERROR METRICS SUMMARY\n",
    "        {'='*40}\n",
    "\n",
    "        RMSE:        {np.sqrt(np.mean(residuals**2)):.6f}\n",
    "        MAE:         {abs_errors.mean():.6f}\n",
    "        Median AE:   {np.median(abs_errors):.6f}\n",
    "        Max Error:   {abs_errors.max():.6f}\n",
    "\n",
    "        Mean Error:  {residuals.mean():.6f}\n",
    "        Std Error:   {residuals.std():.6f}\n",
    "\n",
    "        MAPE:        {np.mean(np.abs(pct_errors)):.2f}%\n",
    "\n",
    "        Error Range: [{residuals.min():.4f}, {residuals.max():.4f}]\n",
    "\n",
    "        Errors > 2σ: {(abs_errors > 2*residuals.std()).sum()}\n",
    "                     ({100*(abs_errors > 2*residuals.std()).sum()/len(residuals):.1f}%)\n",
    "        \"\"\"\n",
    "\n",
    "        ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes,\n",
    "                fontsize=11, verticalalignment='top', family='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "        plt.suptitle('Error Distribution Analysis', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_feature_importance(self, features_df, top_n=20):\n",
    "        \"\"\"\n",
    "        Plot feature importance from the fitted models\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'mean_model') or self.mean_model is None:\n",
    "            print(\"No model fitted yet. Run fit() first.\")\n",
    "            return\n",
    "\n",
    "        # Get feature importance\n",
    "        feature_cols = [col for col in features_df.columns if col != 'target']\n",
    "\n",
    "        # Train a final model on all data to get feature importance\n",
    "        X = features_df[feature_cols].values\n",
    "        y = features_df['target'].values\n",
    "\n",
    "        model = XGBRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "        model.fit(X, y, verbose=False)\n",
    "\n",
    "        importance = model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        # Top N features\n",
    "        top_features = feature_importance.head(top_n)\n",
    "        ax1 = axes[0]\n",
    "        ax1.barh(range(len(top_features)), top_features['importance'])\n",
    "        ax1.set_yticks(range(len(top_features)))\n",
    "        ax1.set_yticklabels(top_features['feature'])\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_title(f'Top {top_n} Feature Importance', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Importance Score')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        # Feature importance by category\n",
    "        ax2 = axes[1]\n",
    "\n",
    "        # Categorize features\n",
    "        categories = {\n",
    "            'Lag': feature_importance[feature_importance['feature'].str.contains('lag_')]['importance'].sum(),\n",
    "            'Rolling Stats': feature_importance[feature_importance['feature'].str.contains('rolling_')]['importance'].sum(),\n",
    "            'Volatility': feature_importance[feature_importance['feature'].str.contains('vol')]['importance'].sum(),\n",
    "            'STL': feature_importance[feature_importance['feature'].str.contains('trend|seasonal')]['importance'].sum(),\n",
    "            'Time': feature_importance[feature_importance['feature'].str.contains('day|month|quarter')]['importance'].sum()\n",
    "        }\n",
    "\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "        ax2.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "        ax2.set_title('Feature Importance by Category', fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return feature_importance\n",
    "\n",
    "    def fit(self, series, use_quantile=False):\n",
    "        \"\"\"\n",
    "        Complete fitting pipeline\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING HETEROSCEDASTICITY-AWARE XGBOOST PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Step 1: Make stationary\n",
    "        series_stationary = self.make_stationary(series)\n",
    "\n",
    "        # Step 2: Find optimal lags\n",
    "        self.find_optimal_lags(series_stationary)\n",
    "\n",
    "        # Step 3: STL decomposition\n",
    "        self.stl_decomposition(series_stationary)\n",
    "\n",
    "        # Step 4: Create features\n",
    "        features_df = self.create_features(series_stationary, include_volatility=True)\n",
    "\n",
    "        # Step 5: Test heteroscedasticity (on a preliminary model)\n",
    "        X_temp = features_df.drop('target', axis=1).values\n",
    "        y_temp = features_df['target'].values\n",
    "\n",
    "        temp_model = XGBRegressor(n_estimators=50, max_depth=3, random_state=42)\n",
    "        temp_model.fit(X_temp[:len(X_temp)//2], y_temp[:len(y_temp)//2], verbose=False)\n",
    "        temp_residuals = y_temp[:len(y_temp)//2] - temp_model.predict(X_temp[:len(X_temp)//2])\n",
    "\n",
    "        is_hetero = self.test_heteroscedasticity(temp_residuals, X_temp[:len(X_temp)//2])\n",
    "\n",
    "        if is_hetero:\n",
    "            print(\"\\n*** HETEROSCEDASTICITY DETECTED - Using volatility modeling ***\")\n",
    "\n",
    "        # Step 6: Rolling window cross-validation\n",
    "        cv_results = self.rolling_window_cv(\n",
    "            features_df,\n",
    "            use_quantile=use_quantile,\n",
    "            initial_train_size=self.seasonal_period\n",
    "        )\n",
    "\n",
    "        return cv_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic commodity price data with heteroscedasticity\n",
    "    np.random.seed(42)\n",
    "    n = 500\n",
    "    t = np.arange(n)\n",
    "\n",
    "    # Trend + Seasonality + Heteroscedastic noise\n",
    "    trend = 0.05 * t\n",
    "    seasonal = 10 * np.sin(2 * np.pi * t / 12)\n",
    "\n",
    "    # Heteroscedastic volatility (GARCH-like)\n",
    "    volatility = np.zeros(n)\n",
    "    volatility[0] = 1.0\n",
    "    for i in range(1, n):\n",
    "        volatility[i] = 0.1 + 0.85 * volatility[i-1] + 0.1 * np.random.randn()**2\n",
    "\n",
    "    noise = volatility * np.random.randn(n)\n",
    "    price = 100 + trend + seasonal + noise\n",
    "\n",
    "    # Create time series\n",
    "    dates = pd.date_range('2020-01-01', periods=n, freq='D')\n",
    "    series = pd.Series(price, index=dates, name='price')\n",
    "\n",
    "    print(\"Sample data shape:\", series.shape)\n",
    "    print(\"Sample data (first 5):\", series.head())\n",
    "\n",
    "    # Initialize and fit pipeline\n",
    "    pipeline = HeteroscedasticXGBoostPipeline(seasonal_period=12, max_lags=20)\n",
    "\n",
    "    # Option 1: Quantile regression (recommended for heteroscedasticity)\n",
    "    results_quantile = pipeline.fit(series, use_quantile=True)\n",
    "\n",
    "    # Option 2: Two-model approach (mean + volatility)\n",
    "    # results_two_model = pipeline.fit(series, use_quantile=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Visualize diagnostics\n",
    "    pipeline.plot_diagnostics(series, results_quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11037,
     "status": "ok",
     "timestamp": 1762136615835,
     "user": {
      "displayName": "Sanchit Mishra",
      "userId": "13460189953341801234"
     },
     "user_tz": -330
    },
    "id": "tjKlaNVJa-4w",
    "outputId": "3a58d66a-c582-4cb2-d573-1a61ef7cd0b2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Heteroscedasticity-Aware XGBoost Pipeline for Commodity Price Prediction\n",
    "Includes: Stationarity testing, STL decomposition, volatility modeling, rolling window CV\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, acf, pacf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from scipy.stats import boxcox, jarque_bera\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class HeteroscedasticXGBoostPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for commodity price prediction with heteroscedasticity handling\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seasonal_period=12, max_lags=20):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        seasonal_period : int\n",
    "            Expected seasonality period (12 for monthly, 252 for daily trading days)\n",
    "            Note: Will be adjusted to odd number >= 3 for STL decomposition\n",
    "        max_lags : int\n",
    "            Maximum number of lags to consider\n",
    "        \"\"\"\n",
    "        self.seasonal_period = seasonal_period\n",
    "        self.max_lags = max_lags\n",
    "        self.differencing_order = 0\n",
    "        self.transformation = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.optimal_lags = []\n",
    "\n",
    "        # Models\n",
    "        self.mean_model = None\n",
    "        self.vol_model = None\n",
    "        self.quantile_models = {}\n",
    "\n",
    "        # Results storage\n",
    "        self.stationarity_results = {}\n",
    "        self.heteroscedasticity_results = {}\n",
    "        self.stl_components = None\n",
    "\n",
    "    def test_stationarity(self, series, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Perform ADF and KPSS tests for stationarity\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STATIONARITY TESTS ===\")\n",
    "\n",
    "        # ADF Test (H0: unit root present - non-stationary)\n",
    "        adf_result = adfuller(series.dropna(), autolag='AIC')\n",
    "        print(f\"\\nAugmented Dickey-Fuller Test:\")\n",
    "        print(f\"  ADF Statistic: {adf_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "        print(f\"  Critical Values: {adf_result[4]}\")\n",
    "        adf_stationary = adf_result[1] < alpha\n",
    "        print(f\"  Result: {'STATIONARY' if adf_stationary else 'NON-STATIONARY'}\")\n",
    "\n",
    "        # KPSS Test (H0: series is stationary)\n",
    "        kpss_result = kpss(series.dropna(), regression='ct', nlags='auto')\n",
    "        print(f\"\\nKPSS Test:\")\n",
    "        print(f\"  KPSS Statistic: {kpss_result[0]:.4f}\")\n",
    "        print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
    "        print(f\"  Critical Values: {kpss_result[3]}\")\n",
    "        kpss_stationary = kpss_result[1] > alpha\n",
    "        print(f\"  Result: {'STATIONARY' if kpss_stationary else 'NON-STATIONARY'}\")\n",
    "\n",
    "        self.stationarity_results = {\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'adf_stationary': adf_stationary,\n",
    "            'kpss_statistic': kpss_result[0],\n",
    "            'kpss_pvalue': kpss_result[1],\n",
    "            'kpss_stationary': kpss_stationary,\n",
    "            'is_stationary': adf_stationary and kpss_stationary\n",
    "        }\n",
    "\n",
    "        return self.stationarity_results['is_stationary']\n",
    "\n",
    "    def make_stationary(self, series):\n",
    "        \"\"\"\n",
    "        Apply transformations to make series stationary\n",
    "        \"\"\"\n",
    "        original_series = series.copy()\n",
    "\n",
    "        if not self.test_stationarity(series):\n",
    "            print(\"\\n=== MAKING SERIES STATIONARY ===\")\n",
    "\n",
    "            # Try log transformation first (variance stabilization)\n",
    "            if (series > 0).all():\n",
    "                series_log = np.log(series)\n",
    "                if self.test_stationarity(series_log):\n",
    "                    print(\"Log transformation sufficient\")\n",
    "                    self.transformation = 'log'\n",
    "                    return series_log\n",
    "\n",
    "            # Apply first differencing\n",
    "            series_diff = series.diff().dropna()\n",
    "            if self.test_stationarity(series_diff):\n",
    "                print(\"First differencing sufficient\")\n",
    "                self.differencing_order = 1\n",
    "                self.transformation = 'diff'\n",
    "                return series_diff\n",
    "\n",
    "            # Apply second differencing if needed\n",
    "            series_diff2 = series_diff.diff().dropna()\n",
    "            if self.test_stationarity(series_diff2):\n",
    "                print(\"Second differencing required\")\n",
    "                self.differencing_order = 2\n",
    "                self.transformation = 'diff2'\n",
    "                return series_diff2\n",
    "\n",
    "            print(\"Warning: Could not achieve stationarity. Proceeding with first difference.\")\n",
    "            self.differencing_order = 1\n",
    "            self.transformation = 'diff'\n",
    "            return series.diff().dropna()\n",
    "\n",
    "        print(\"Series is already stationary\")\n",
    "        self.transformation = 'none'\n",
    "        return series\n",
    "\n",
    "    def find_optimal_lags(self, series, method='acf', threshold=0.2):\n",
    "        \"\"\"\n",
    "        Determine optimal lag order using ACF/PACF\n",
    "        \"\"\"\n",
    "        print(\"\\n=== FINDING OPTIMAL LAGS ===\")\n",
    "\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        # ACF\n",
    "        acf_values = acf(series_clean, nlags=self.max_lags, fft=False)\n",
    "        significant_acf = np.where(np.abs(acf_values[1:]) > threshold)[0] + 1\n",
    "\n",
    "        # PACF\n",
    "        pacf_values = pacf(series_clean, nlags=self.max_lags, method='ols')\n",
    "        significant_pacf = np.where(np.abs(pacf_values[1:]) > threshold)[0] + 1\n",
    "\n",
    "        if method == 'acf':\n",
    "            self.optimal_lags = significant_acf[:10].tolist()  # Top 10 lags\n",
    "        elif method == 'pacf':\n",
    "            self.optimal_lags = significant_pacf[:10].tolist()\n",
    "        else:  # both\n",
    "            self.optimal_lags = sorted(list(set(significant_acf[:10].tolist() +\n",
    "                                                 significant_pacf[:10].tolist())))\n",
    "\n",
    "        if not self.optimal_lags:\n",
    "            self.optimal_lags = [1, 2, 3, 5, 7]  # Default lags\n",
    "\n",
    "        print(f\"Optimal lags: {self.optimal_lags}\")\n",
    "        return self.optimal_lags\n",
    "\n",
    "    def stl_decomposition(self, series):\n",
    "        \"\"\"\n",
    "        Perform STL decomposition\n",
    "        \"\"\"\n",
    "        print(\"\\n=== STL DECOMPOSITION ===\")\n",
    "\n",
    "        series_clean = series.dropna()\n",
    "\n",
    "        if len(series_clean) < 2 * self.seasonal_period:\n",
    "            print(f\"Warning: Series too short for seasonal decomposition (need >= {2*self.seasonal_period})\")\n",
    "            return None\n",
    "\n",
    "        # Ensure seasonal parameter is odd and >= 3\n",
    "        seasonal_param = self.seasonal_period\n",
    "        if seasonal_param % 2 == 0:\n",
    "            seasonal_param += 1\n",
    "        if seasonal_param < 3:\n",
    "            seasonal_param = 3\n",
    "\n",
    "        print(f\"Using seasonal parameter: {seasonal_param}\")\n",
    "\n",
    "        try:\n",
    "            stl = STL(series_clean, seasonal=seasonal_param, robust=True)\n",
    "            result = stl.fit()\n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "            print(\"Proceeding without STL components\")\n",
    "            return None\n",
    "\n",
    "        self.stl_components = {\n",
    "            'trend': result.trend,\n",
    "            'seasonal': result.seasonal,\n",
    "            'resid': result.resid\n",
    "        }\n",
    "\n",
    "        print(f\"Trend strength: {1 - result.resid.var() / (result.trend + result.resid).var():.4f}\")\n",
    "        print(f\"Seasonal strength: {1 - result.resid.var() / (result.seasonal + result.resid).var():.4f}\")\n",
    "\n",
    "        return self.stl_components\n",
    "\n",
    "    def test_heteroscedasticity(self, residuals, features):\n",
    "        \"\"\"\n",
    "        Test for heteroscedasticity using Breusch-Pagan and White tests\n",
    "        \"\"\"\n",
    "        print(\"\\n=== HETEROSCEDASTICITY TESTS ===\")\n",
    "\n",
    "        # Breusch-Pagan test\n",
    "        try:\n",
    "            bp_stat, bp_pval, _, _ = het_breuschpagan(residuals, features)\n",
    "            print(f\"\\nBreusch-Pagan Test:\")\n",
    "            print(f\"  LM Statistic: {bp_stat:.4f}\")\n",
    "            print(f\"  p-value: {bp_pval:.4f}\")\n",
    "            bp_hetero = bp_pval < 0.05\n",
    "            print(f\"  Result: {'HETEROSCEDASTIC' if bp_hetero else 'HOMOSCEDASTIC'}\")\n",
    "        except:\n",
    "            bp_hetero = True\n",
    "            bp_pval = 0.0\n",
    "            print(\"Breusch-Pagan test failed\")\n",
    "\n",
    "        # White test\n",
    "        try:\n",
    "            white_stat, white_pval, _, _ = het_white(residuals, features)\n",
    "            print(f\"\\nWhite Test:\")\n",
    "            print(f\"  LM Statistic: {white_stat:.4f}\")\n",
    "            print(f\"  p-value: {white_pval:.4f}\")\n",
    "            white_hetero = white_pval < 0.05\n",
    "            print(f\"  Result: {'HETEROSCEDASTIC' if white_hetero else 'HOMOSCEDASTIC'}\")\n",
    "        except:\n",
    "            white_hetero = True\n",
    "            white_pval = 0.0\n",
    "            print(\"White test failed\")\n",
    "\n",
    "        self.heteroscedasticity_results = {\n",
    "            'bp_pvalue': bp_pval,\n",
    "            'white_pvalue': white_pval,\n",
    "            'is_heteroscedastic': bp_hetero or white_hetero\n",
    "        }\n",
    "\n",
    "        return self.heteroscedasticity_results['is_heteroscedastic']\n",
    "\n",
    "    def create_features(self, series, include_volatility=True):\n",
    "        \"\"\"\n",
    "        Create feature matrix with lags, rolling statistics, and volatility features\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'target': series})\n",
    "\n",
    "        # Lag features\n",
    "        for lag in self.optimal_lags:\n",
    "            df[f'lag_{lag}'] = df['target'].shift(lag)\n",
    "\n",
    "        # Rolling statistics\n",
    "        for window in [5, 10, 20]:\n",
    "            df[f'rolling_mean_{window}'] = df['target'].rolling(window).mean()\n",
    "            df[f'rolling_std_{window}'] = df['target'].rolling(window).std()\n",
    "            df[f'rolling_min_{window}'] = df['target'].rolling(window).min()\n",
    "            df[f'rolling_max_{window}'] = df['target'].rolling(window).max()\n",
    "\n",
    "        # Volatility features (if enabled)\n",
    "        if include_volatility:\n",
    "            # Historical volatility at multiple horizons\n",
    "            for window in [5, 10, 20, 60]:\n",
    "                df[f'volatility_{window}'] = df['target'].rolling(window).std()\n",
    "\n",
    "            # Realized volatility (if returns)\n",
    "            df['realized_vol'] = df['target'].rolling(20).std() * np.sqrt(252)\n",
    "\n",
    "            # Volatility of volatility\n",
    "            df['vol_of_vol'] = df['volatility_20'].rolling(20).std()\n",
    "\n",
    "            # Volatility regime (high/low)\n",
    "            vol_median = df['volatility_20'].rolling(60).median()\n",
    "            df['high_vol_regime'] = (df['volatility_20'] > vol_median).astype(int)\n",
    "\n",
    "            # Range-based volatility proxy\n",
    "            df['range_vol'] = (df['rolling_max_20'] - df['rolling_min_20']) / df['rolling_mean_20']\n",
    "\n",
    "        # STL components (if available)\n",
    "        if self.stl_components is not None:\n",
    "            df['trend'] = self.stl_components['trend']\n",
    "            df['seasonal'] = self.stl_components['seasonal']\n",
    "\n",
    "        # Time features\n",
    "        if isinstance(df.index, pd.DatetimeIndex):\n",
    "            df['day_of_week'] = df.index.dayofweek\n",
    "            df['month'] = df.index.month\n",
    "            df['quarter'] = df.index.quarter\n",
    "            df['day_of_month'] = df.index.day\n",
    "\n",
    "        # Drop NaN rows\n",
    "        df = df.dropna()\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_volatility_weights(self, series, window=20, method='inverse_variance'):\n",
    "        \"\"\"\n",
    "        Create sample weights based on volatility (optimized version)\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if pandas Series\n",
    "        if isinstance(series, pd.Series):\n",
    "            values = series.values\n",
    "        else:\n",
    "            values = np.array(series)\n",
    "\n",
    "        if method == 'inverse_variance':\n",
    "            # Use pandas rolling for speed (with proper handling)\n",
    "            series_pd = pd.Series(values)\n",
    "            rolling_var = series_pd.rolling(window, min_periods=1).var()\n",
    "\n",
    "            # Clean up variance\n",
    "            rolling_var = rolling_var.fillna(rolling_var.median())\n",
    "            rolling_var = np.clip(rolling_var, 1e-10, None)  # Floor at small positive\n",
    "\n",
    "            # Inverse variance weights\n",
    "            weights = 1.0 / rolling_var.values\n",
    "\n",
    "        elif method == 'exponential':\n",
    "            half_life = window\n",
    "            weights = np.exp(-np.log(2) * np.arange(len(values))[::-1] / half_life)\n",
    "        else:\n",
    "            weights = np.ones(len(values))\n",
    "\n",
    "        # Fast cleanup\n",
    "        weights = np.clip(weights, 1e-8, 1e8)  # Clip to reasonable range\n",
    "        weights[~np.isfinite(weights)] = 1.0  # Replace NaN/Inf\n",
    "\n",
    "        # Normalize\n",
    "        weights = weights / np.sum(weights) * len(weights)\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def rolling_window_cv(self, data, target_col='target',\n",
    "                          initial_train_size=None, step_size=1,\n",
    "                          use_quantile=False, quantiles=[0.1, 0.5, 0.9],\n",
    "                          n_estimators=100, max_depth=4, verbose=True):\n",
    "        \"\"\"\n",
    "        Perform rolling window cross-validation (optimized)\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : DataFrame\n",
    "            Feature matrix with target column\n",
    "        target_col : str\n",
    "            Name of target column\n",
    "        initial_train_size : int or None\n",
    "            Initial training window size (if None, uses one seasonal period)\n",
    "        step_size : int\n",
    "            Number of periods to roll forward\n",
    "        use_quantile : bool\n",
    "            Whether to use quantile regression\n",
    "        quantiles : list\n",
    "            Quantiles to predict if use_quantile=True\n",
    "        n_estimators : int\n",
    "            Number of trees (lower = faster)\n",
    "        max_depth : int\n",
    "            Tree depth (lower = faster)\n",
    "        verbose : bool\n",
    "            Print progress\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n=== ROLLING WINDOW CROSS-VALIDATION ===\")\n",
    "\n",
    "        if initial_train_size is None:\n",
    "            initial_train_size = self.seasonal_period\n",
    "\n",
    "        feature_cols = [col for col in data.columns if col != target_col]\n",
    "        X = data[feature_cols].values\n",
    "        y = data[target_col].values\n",
    "\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        prediction_intervals = []\n",
    "        fold_metrics = []\n",
    "\n",
    "        n_folds = (len(data) - initial_train_size) // step_size\n",
    "\n",
    "        for fold in range(n_folds):\n",
    "            train_end = initial_train_size + fold * step_size\n",
    "            test_start = train_end\n",
    "            test_end = min(test_start + step_size, len(data))\n",
    "\n",
    "            if test_end >= len(data):\n",
    "                break\n",
    "\n",
    "            X_train, y_train = X[:train_end], y[:train_end]\n",
    "            X_test, y_test = X[test_start:test_end], y[test_start:test_end]\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\nFold {fold + 1}/{n_folds}: Train={len(X_train)}, Test={len(X_test)}\")\n",
    "\n",
    "            # Simplified: Use uniform weights for speed (comment out for weighted version)\n",
    "            # weights = np.ones(len(y_train))\n",
    "\n",
    "            # Or use exponential weights (faster than inverse variance)\n",
    "            weights = self.create_volatility_weights(\n",
    "                y_train,\n",
    "                window=min(20, len(y_train)//2),\n",
    "                method='exponential'  # Much faster than inverse_variance\n",
    "            )\n",
    "\n",
    "            if use_quantile:\n",
    "                # Quantile regression approach\n",
    "                fold_predictions = {}\n",
    "\n",
    "                for q in quantiles:\n",
    "                    model = XGBRegressor(\n",
    "                        objective='reg:quantileerror',\n",
    "                        quantile_alpha=q,\n",
    "                        n_estimators=n_estimators,  # Reduced from 100\n",
    "                        max_depth=max_depth,        # Reduced from 4\n",
    "                        learning_rate=0.1,          # Increased from 0.05 for speed\n",
    "                        subsample=0.8,\n",
    "                        colsample_bytree=0.8,\n",
    "                        n_jobs=-1,                  # Use all cores\n",
    "                        random_state=42,\n",
    "                        tree_method='hist'          # Faster tree building\n",
    "                    )\n",
    "                    model.fit(X_train, y_train, sample_weight=weights, verbose=0)\n",
    "                    fold_predictions[q] = model.predict(X_test)\n",
    "\n",
    "                # Use median as point prediction\n",
    "                pred = fold_predictions[0.5]\n",
    "                lower = fold_predictions[quantiles[0]]\n",
    "                upper = fold_predictions[quantiles[-1]]\n",
    "\n",
    "                prediction_intervals.extend(list(zip(lower, upper)))\n",
    "\n",
    "            else:\n",
    "                # Two-model approach: Mean + Volatility\n",
    "\n",
    "                # Model 1: Mean prediction\n",
    "                mean_model = XGBRegressor(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                    tree_method='hist'\n",
    "                )\n",
    "                mean_model.fit(X_train, y_train, sample_weight=weights, verbose=0)\n",
    "                pred_mean = mean_model.predict(X_test)\n",
    "\n",
    "                # Model 2: Volatility prediction (simplified)\n",
    "                train_residuals = y_train - mean_model.predict(X_train)\n",
    "                train_vol = pd.Series(train_residuals).rolling(20, min_periods=5).std().fillna(\n",
    "                    pd.Series(train_residuals).std()\n",
    "                ).values\n",
    "\n",
    "                vol_model = XGBRegressor(\n",
    "                    n_estimators=n_estimators//2,  # Half the trees\n",
    "                    max_depth=3,\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                    tree_method='hist'\n",
    "                )\n",
    "                vol_model.fit(X_train, train_vol, verbose=0)\n",
    "                pred_vol = vol_model.predict(X_test)\n",
    "\n",
    "                pred = pred_mean\n",
    "                lower = pred_mean - 1.96 * pred_vol\n",
    "                upper = pred_mean + 1.96 * pred_vol\n",
    "\n",
    "                prediction_intervals.extend(list(zip(lower, upper)))\n",
    "\n",
    "            predictions.extend(pred)\n",
    "            actuals.extend(y_test)\n",
    "\n",
    "            # Calculate fold metrics\n",
    "            rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "            mae = np.mean(np.abs(pred - y_test))\n",
    "\n",
    "            # Coverage of prediction intervals\n",
    "            in_interval = np.sum((y_test >= lower) & (y_test <= upper))\n",
    "            coverage = in_interval / len(y_test)\n",
    "\n",
    "            fold_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'coverage': coverage\n",
    "            })\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  RMSE: {rmse:.4f}, MAE: {mae:.4f}, Coverage: {coverage:.2%}\")\n",
    "\n",
    "        results = {\n",
    "            'predictions': np.array(predictions),\n",
    "            'actuals': np.array(actuals),\n",
    "            'prediction_intervals': prediction_intervals,\n",
    "            'fold_metrics': pd.DataFrame(fold_metrics)\n",
    "        }\n",
    "\n",
    "        # Overall metrics\n",
    "        overall_rmse = np.sqrt(np.mean((results['predictions'] - results['actuals']) ** 2))\n",
    "        overall_mae = np.mean(np.abs(results['predictions'] - results['actuals']))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== OVERALL CROSS-VALIDATION RESULTS ===\")\n",
    "            print(f\"RMSE: {overall_rmse:.4f}\")\n",
    "            print(f\"MAE: {overall_mae:.4f}\")\n",
    "            print(f\"Mean Coverage: {results['fold_metrics']['coverage'].mean():.2%}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_diagnostics(self, original_series, cv_results, save_path=None):\n",
    "        \"\"\"\n",
    "        Comprehensive diagnostic plots for model evaluation\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        original_series : pd.Series\n",
    "            Original time series data\n",
    "        cv_results : dict\n",
    "            Results from rolling_window_cv\n",
    "        save_path : str, optional\n",
    "            Path to save figures\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        gs = fig.add_gridspec(5, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "        # 1. Original Series with Trend and Seasonality\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax1.plot(original_series.index, original_series.values,\n",
    "                 label='Original', alpha=0.7, linewidth=1)\n",
    "        if self.stl_components is not None:\n",
    "            ax1.plot(self.stl_components['trend'].index,\n",
    "                    self.stl_components['trend'].values,\n",
    "                    label='Trend', linewidth=2, color='red')\n",
    "        ax1.set_title('Original Time Series with Trend', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('Price')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. STL Decomposition Components\n",
    "        if self.stl_components is not None:\n",
    "            ax2 = fig.add_subplot(gs[1, 0])\n",
    "            ax2.plot(self.stl_components['trend'].index,\n",
    "                    self.stl_components['trend'].values, color='red')\n",
    "            ax2.set_title('Trend Component', fontsize=12, fontweight='bold')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            ax3 = fig.add_subplot(gs[1, 1])\n",
    "            ax3.plot(self.stl_components['seasonal'].index,\n",
    "                    self.stl_components['seasonal'].values, color='green')\n",
    "            ax3.set_title('Seasonal Component', fontsize=12, fontweight='bold')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "\n",
    "            ax4 = fig.add_subplot(gs[1, 2])\n",
    "            ax4.plot(self.stl_components['resid'].index,\n",
    "                    self.stl_components['resid'].values, color='purple', alpha=0.6)\n",
    "            ax4.set_title('Residual Component', fontsize=12, fontweight='bold')\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Predictions vs Actuals\n",
    "        ax5 = fig.add_subplot(gs[2, :])\n",
    "        test_indices = range(len(cv_results['actuals']))\n",
    "        ax5.plot(test_indices, cv_results['actuals'],\n",
    "                label='Actual', color='blue', linewidth=2, alpha=0.7)\n",
    "        ax5.plot(test_indices, cv_results['predictions'],\n",
    "                label='Predicted', color='red', linewidth=2, alpha=0.7)\n",
    "\n",
    "        # Add prediction intervals\n",
    "        if cv_results['prediction_intervals']:\n",
    "            intervals = np.array(cv_results['prediction_intervals'])\n",
    "            ax5.fill_between(test_indices, intervals[:, 0], intervals[:, 1],\n",
    "                           alpha=0.2, color='red', label='95% Prediction Interval')\n",
    "\n",
    "        ax5.set_title('Predictions vs Actuals (Cross-Validation)',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax5.set_xlabel('Test Sample Index')\n",
    "        ax5.set_ylabel('Value')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Residuals Analysis\n",
    "        residuals = cv_results['predictions'] - cv_results['actuals']\n",
    "\n",
    "        ax6 = fig.add_subplot(gs[3, 0])\n",
    "        ax6.scatter(cv_results['predictions'], residuals, alpha=0.5, s=20)\n",
    "        ax6.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax6.set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "        ax6.set_xlabel('Predicted Values')\n",
    "        ax6.set_ylabel('Residuals')\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add LOESS smooth to detect patterns\n",
    "        from scipy.signal import savgol_filter\n",
    "        if len(residuals) > 50:\n",
    "            sorted_idx = np.argsort(cv_results['predictions'])\n",
    "            smoothed = savgol_filter(residuals[sorted_idx],\n",
    "                                    min(51, len(residuals)//2*2-1), 3)\n",
    "            ax6.plot(cv_results['predictions'][sorted_idx], smoothed,\n",
    "                    color='orange', linewidth=2, label='Smoothed')\n",
    "            ax6.legend()\n",
    "\n",
    "        # 5. Residuals Histogram\n",
    "        ax7 = fig.add_subplot(gs[3, 1])\n",
    "        ax7.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "        ax7.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax7.set_title('Residuals Distribution', fontsize=12, fontweight='bold')\n",
    "        ax7.set_xlabel('Residual Value')\n",
    "        ax7.set_ylabel('Frequency')\n",
    "\n",
    "        # Add normality statistics\n",
    "        from scipy.stats import shapiro\n",
    "        _, p_value = shapiro(residuals)\n",
    "        ax7.text(0.05, 0.95, f'Shapiro-Wilk p={p_value:.4f}',\n",
    "                transform=ax7.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        ax7.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6. Q-Q Plot\n",
    "        ax8 = fig.add_subplot(gs[3, 2])\n",
    "        from scipy.stats import probplot\n",
    "        probplot(residuals, dist=\"norm\", plot=ax8)\n",
    "        ax8.set_title('Q-Q Plot', fontsize=12, fontweight='bold')\n",
    "        ax8.grid(True, alpha=0.3)\n",
    "\n",
    "        # 7. ACF of Residuals\n",
    "        ax9 = fig.add_subplot(gs[4, 0])\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plot_acf(residuals, lags=min(40, len(residuals)//2), ax=ax9, alpha=0.05)\n",
    "        ax9.set_title('ACF of Residuals', fontsize=12, fontweight='bold')\n",
    "        ax9.set_xlabel('Lag')\n",
    "\n",
    "        # 8. Rolling RMSE\n",
    "        ax10 = fig.add_subplot(gs[4, 1])\n",
    "        fold_metrics = cv_results['fold_metrics']\n",
    "        ax10.plot(fold_metrics['fold'], fold_metrics['rmse'],\n",
    "                 marker='o', linewidth=2, markersize=8)\n",
    "        ax10.set_title('RMSE by Fold', fontsize=12, fontweight='bold')\n",
    "        ax10.set_xlabel('Fold')\n",
    "        ax10.set_ylabel('RMSE')\n",
    "        ax10.grid(True, alpha=0.3)\n",
    "\n",
    "        # 9. Coverage Analysis\n",
    "        ax11 = fig.add_subplot(gs[4, 2])\n",
    "        ax11.plot(fold_metrics['fold'], fold_metrics['coverage'],\n",
    "                 marker='s', linewidth=2, markersize=8, color='green')\n",
    "        ax11.axhline(y=0.95, color='red', linestyle='--', linewidth=2,\n",
    "                    label='Target 95%')\n",
    "        ax11.set_title('Prediction Interval Coverage', fontsize=12, fontweight='bold')\n",
    "        ax11.set_xlabel('Fold')\n",
    "        ax11.set_ylabel('Coverage')\n",
    "        ax11.set_ylim([0, 1])\n",
    "        ax11.legend()\n",
    "        ax11.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.suptitle('Comprehensive Model Diagnostics',\n",
    "                    fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"\\nDiagnostics saved to {save_path}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Additional separate plots\n",
    "        self._plot_volatility_analysis(original_series, cv_results)\n",
    "        self._plot_error_distribution(cv_results)\n",
    "\n",
    "    def _plot_volatility_analysis(self, series, cv_results):\n",
    "        \"\"\"\n",
    "        Detailed volatility analysis plots\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "        # 1. Rolling Volatility\n",
    "        returns = series.pct_change().dropna()\n",
    "        rolling_vol_5 = returns.rolling(5).std()\n",
    "        rolling_vol_20 = returns.rolling(20).std()\n",
    "        rolling_vol_60 = returns.rolling(60).std()\n",
    "\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.plot(rolling_vol_5.index, rolling_vol_5, label='5-period', alpha=0.6)\n",
    "        ax1.plot(rolling_vol_20.index, rolling_vol_20, label='20-period', linewidth=2)\n",
    "        ax1.plot(rolling_vol_60.index, rolling_vol_60, label='60-period', linewidth=2)\n",
    "        ax1.set_title('Rolling Volatility (Multiple Windows)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Volatility')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Volatility Clustering\n",
    "        ax2 = axes[0, 1]\n",
    "        abs_returns = np.abs(returns)\n",
    "        ax2.plot(abs_returns.index, abs_returns, alpha=0.5, linewidth=0.5)\n",
    "        ax2.plot(abs_returns.rolling(20).mean().index,\n",
    "                abs_returns.rolling(20).mean(),\n",
    "                color='red', linewidth=2, label='20-period MA')\n",
    "        ax2.set_title('Volatility Clustering (Absolute Returns)',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('|Returns|')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Squared Residuals ACF (ARCH test)\n",
    "        ax3 = axes[1, 0]\n",
    "        residuals = cv_results['predictions'] - cv_results['actuals']\n",
    "        squared_residuals = residuals ** 2\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plot_acf(squared_residuals, lags=min(40, len(squared_residuals)//2),\n",
    "                ax=ax3, alpha=0.05)\n",
    "        ax3.set_title('ACF of Squared Residuals (ARCH Effects)',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "\n",
    "        # 4. Volatility vs Prediction Error\n",
    "        ax4 = axes[1, 1]\n",
    "        # Estimate local volatility\n",
    "        window = 20\n",
    "        local_vol = pd.Series(cv_results['actuals']).rolling(window, min_periods=5).std()\n",
    "        abs_errors = np.abs(residuals)\n",
    "\n",
    "        ax4.scatter(local_vol, abs_errors, alpha=0.5, s=30)\n",
    "        ax4.set_title('Prediction Error vs Local Volatility',\n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Local Volatility (20-period)')\n",
    "        ax4.set_ylabel('Absolute Error')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add trend line\n",
    "        valid_idx = ~np.isnan(local_vol)\n",
    "        if valid_idx.sum() > 1:\n",
    "            z = np.polyfit(local_vol[valid_idx], abs_errors[valid_idx], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax4.plot(local_vol[valid_idx], p(local_vol[valid_idx]),\n",
    "                    \"r--\", linewidth=2, label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "            ax4.legend()\n",
    "\n",
    "        plt.suptitle('Volatility Analysis', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_error_distribution(self, cv_results):\n",
    "        \"\"\"\n",
    "        Detailed error distribution analysis\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "        residuals = cv_results['predictions'] - cv_results['actuals']\n",
    "        abs_errors = np.abs(residuals)\n",
    "        pct_errors = 100 * residuals / cv_results['actuals']\n",
    "\n",
    "        # 1. Error over time\n",
    "        ax1 = axes[0, 0]\n",
    "        ax1.plot(residuals, alpha=0.7, linewidth=1)\n",
    "        ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax1.fill_between(range(len(residuals)), -2*residuals.std(),\n",
    "                        2*residuals.std(), alpha=0.2, color='red')\n",
    "        ax1.set_title('Residuals Over Time', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Residual')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Absolute Error Distribution\n",
    "        ax2 = axes[0, 1]\n",
    "        ax2.hist(abs_errors, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "        ax2.axvline(x=abs_errors.mean(), color='red', linestyle='--',\n",
    "                   linewidth=2, label=f'Mean: {abs_errors.mean():.4f}')\n",
    "        ax2.axvline(x=np.median(abs_errors), color='green', linestyle='--',\n",
    "                   linewidth=2, label=f'Median: {np.median(abs_errors):.4f}')\n",
    "        ax2.set_title('Absolute Error Distribution', fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Absolute Error')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Percentage Error Distribution\n",
    "        ax3 = axes[0, 2]\n",
    "        ax3.hist(pct_errors, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "        ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax3.set_title('Percentage Error Distribution', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Error (%)')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Error by Prediction Magnitude\n",
    "        ax4 = axes[1, 0]\n",
    "        pred_bins = pd.qcut(cv_results['predictions'], q=10, duplicates='drop')\n",
    "        error_by_bin = pd.DataFrame({\n",
    "            'pred_bin': pred_bins,\n",
    "            'abs_error': abs_errors\n",
    "        }).groupby('pred_bin')['abs_error'].agg(['mean', 'std'])\n",
    "\n",
    "        x_pos = range(len(error_by_bin))\n",
    "        ax4.bar(x_pos, error_by_bin['mean'], yerr=error_by_bin['std'],\n",
    "               alpha=0.7, capsize=5)\n",
    "        ax4.set_title('Error by Prediction Magnitude', fontsize=12, fontweight='bold')\n",
    "        ax4.set_xlabel('Prediction Bin (Deciles)')\n",
    "        ax4.set_ylabel('Mean Absolute Error')\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # 5. Cumulative Error\n",
    "        ax5 = axes[1, 1]\n",
    "        cumulative_error = np.cumsum(residuals)\n",
    "        ax5.plot(cumulative_error, linewidth=2)\n",
    "        ax5.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax5.set_title('Cumulative Error (Bias Detection)', fontsize=12, fontweight='bold')\n",
    "        ax5.set_ylabel('Cumulative Error')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "\n",
    "        # 6. Error Metrics Summary\n",
    "        ax6 = axes[1, 2]\n",
    "        ax6.axis('off')\n",
    "\n",
    "        metrics_text = f\"\"\"\n",
    "        ERROR METRICS SUMMARY\n",
    "        {'='*40}\n",
    "\n",
    "        RMSE:        {np.sqrt(np.mean(residuals**2)):.6f}\n",
    "        MAE:         {abs_errors.mean():.6f}\n",
    "        Median AE:   {np.median(abs_errors):.6f}\n",
    "        Max Error:   {abs_errors.max():.6f}\n",
    "\n",
    "        Mean Error:  {residuals.mean():.6f}\n",
    "        Std Error:   {residuals.std():.6f}\n",
    "\n",
    "        MAPE:        {np.mean(np.abs(pct_errors)):.2f}%\n",
    "\n",
    "        Error Range: [{residuals.min():.4f}, {residuals.max():.4f}]\n",
    "\n",
    "        Errors > 2σ: {(abs_errors > 2*residuals.std()).sum()}\n",
    "                     ({100*(abs_errors > 2*residuals.std()).sum()/len(residuals):.1f}%)\n",
    "        \"\"\"\n",
    "\n",
    "        ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes,\n",
    "                fontsize=11, verticalalignment='top', family='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "        plt.suptitle('Error Distribution Analysis', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_feature_importance(self, features_df, top_n=20):\n",
    "        \"\"\"\n",
    "        Plot feature importance from the fitted models\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'mean_model') or self.mean_model is None:\n",
    "            print(\"No model fitted yet. Run fit() first.\")\n",
    "            return\n",
    "\n",
    "        # Get feature importance\n",
    "        feature_cols = [col for col in features_df.columns if col != 'target']\n",
    "\n",
    "        # Train a final model on all data to get feature importance\n",
    "        X = features_df[feature_cols].values\n",
    "        y = features_df['target'].values\n",
    "\n",
    "        model = XGBRegressor(n_estimators=100, max_depth=4, random_state=42)\n",
    "        model.fit(X, y, verbose=False)\n",
    "\n",
    "        importance = model.feature_importances_\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        # Plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        # Top N features\n",
    "        top_features = feature_importance.head(top_n)\n",
    "        ax1 = axes[0]\n",
    "        ax1.barh(range(len(top_features)), top_features['importance'])\n",
    "        ax1.set_yticks(range(len(top_features)))\n",
    "        ax1.set_yticklabels(top_features['feature'])\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_title(f'Top {top_n} Feature Importance', fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Importance Score')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "        # Feature importance by category\n",
    "        ax2 = axes[1]\n",
    "\n",
    "        # Categorize features\n",
    "        categories = {\n",
    "            'Lag': feature_importance[feature_importance['feature'].str.contains('lag_')]['importance'].sum(),\n",
    "            'Rolling Stats': feature_importance[feature_importance['feature'].str.contains('rolling_')]['importance'].sum(),\n",
    "            'Volatility': feature_importance[feature_importance['feature'].str.contains('vol')]['importance'].sum(),\n",
    "            'STL': feature_importance[feature_importance['feature'].str.contains('trend|seasonal')]['importance'].sum(),\n",
    "            'Time': feature_importance[feature_importance['feature'].str.contains('day|month|quarter')]['importance'].sum()\n",
    "        }\n",
    "\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "        ax2.pie(categories.values(), labels=categories.keys(), autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "        ax2.set_title('Feature Importance by Category', fontsize=12, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return feature_importance\n",
    "\n",
    "    def fit(self, series, use_quantile=False, fast_mode=True):\n",
    "        \"\"\"\n",
    "        Complete fitting pipeline\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        series : pd.Series\n",
    "            Time series data\n",
    "        use_quantile : bool\n",
    "            Use quantile regression (slower but better intervals)\n",
    "        fast_mode : bool\n",
    "            Use faster settings (fewer trees, larger steps)\n",
    "        \"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING HETEROSCEDASTICITY-AWARE XGBOOST PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Step 1: Make stationary\n",
    "        series_stationary = self.make_stationary(series)\n",
    "\n",
    "        # Step 2: Find optimal lags\n",
    "        self.find_optimal_lags(series_stationary)\n",
    "\n",
    "        # Step 3: STL decomposition\n",
    "        self.stl_decomposition(series_stationary)\n",
    "\n",
    "        # Step 4: Create features\n",
    "        features_df = self.create_features(series_stationary, include_volatility=True)\n",
    "\n",
    "        # Step 5: Test heteroscedasticity (skip in fast mode)\n",
    "        if not fast_mode:\n",
    "            X_temp = features_df.drop('target', axis=1).values\n",
    "            y_temp = features_df['target'].values\n",
    "\n",
    "            temp_model = XGBRegressor(n_estimators=50, max_depth=3, random_state=42, n_jobs=-1)\n",
    "            temp_model.fit(X_temp[:len(X_temp)//2], y_temp[:len(y_temp)//2], verbose=False)\n",
    "            temp_residuals = y_temp[:len(y_temp)//2] - temp_model.predict(X_temp[:len(X_temp)//2])\n",
    "\n",
    "            is_hetero = self.test_heteroscedasticity(temp_residuals, X_temp[:len(X_temp)//2])\n",
    "\n",
    "            if is_hetero:\n",
    "                print(\"\\n*** HETEROSCEDASTICITY DETECTED - Using volatility modeling ***\")\n",
    "\n",
    "        # Step 6: Rolling window cross-validation (optimized)\n",
    "        if fast_mode:\n",
    "            n_estimators = 50    # Reduced trees\n",
    "            step_size = max(1, len(features_df) // 20)  # Larger steps = fewer folds\n",
    "            print(f\"\\nFast mode: {n_estimators} trees, step_size={step_size}\")\n",
    "        else:\n",
    "            n_estimators = 100\n",
    "            step_size = 1\n",
    "\n",
    "        cv_results = self.rolling_window_cv(\n",
    "            features_df,\n",
    "            use_quantile=use_quantile,\n",
    "            initial_train_size=self.seasonal_period,\n",
    "            step_size=step_size,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=4,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        return cv_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate synthetic commodity price data with heteroscedasticity\n",
    "    np.random.seed(42)\n",
    "    n = 500\n",
    "    t = np.arange(n)\n",
    "\n",
    "    # Trend + Seasonality + Heteroscedastic noise\n",
    "    trend = 0.05 * t\n",
    "    seasonal = 10 * np.sin(20 * np.pi * t / 12)\n",
    "\n",
    "    # Heteroscedastic volatility (GARCH-like)\n",
    "    volatility = np.zeros(n)\n",
    "    volatility[0] = 1.0\n",
    "    for i in range(1, n):\n",
    "        volatility[i] = 0.1 + 0.85 * volatility[i-1] + 0.1 * np.random.randn()**2\n",
    "\n",
    "    noise = volatility * np.random.randn(n)\n",
    "    price = 1000 + 2*trend + 1.5*seasonal + noise\n",
    "\n",
    "    # Create time series\n",
    "    dates = pd.date_range('2020-01-01', periods=n, freq='D')\n",
    "    series = pd.Series(price, index=dates, name='price')\n",
    "\n",
    "    print(\"Sample data shape:\", series.shape)\n",
    "    print(\"Sample data (first 5):\", series.head())\n",
    "\n",
    "    # Initialize and fit pipeline\n",
    "    pipeline = HeteroscedasticXGBoostPipeline(seasonal_period=12, max_lags=20)\n",
    "\n",
    "    # Option 1: Quantile regression (recommended for heteroscedasticity)\n",
    "    results_quantile = pipeline.fit(series, use_quantile=True)\n",
    "\n",
    "    # Option 2: Two-model approach (mean + volatility)\n",
    "    # results_two_model = pipeline.fit(series, use_quantile=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Visualize diagnostics\n",
    "    pipeline.plot_diagnostics(series, results_quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntUSCcEBg_Cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1Jkwm7fvqF6LKu6HPusIDc_ZCCIc4qpcd",
     "timestamp": 1762110628744
    },
    {
     "file_id": "1QmYAdu5ASGpk18Tk70qIP5a5AG10tNzI",
     "timestamp": 1762106175757
    },
    {
     "file_id": "1zIQZtsY6KXYWzL16ij7Hgpk5mUlCcQVd",
     "timestamp": 1762103314483
    },
    {
     "file_id": "1Y82xyJ--LzZE3336hu1NOxmStHkI7ybP",
     "timestamp": 1762100538860
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
